<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Arnaud Bailly">
  <meta name="dcterms.date" content="2016-09-05">
  <title>Understanding word2vec</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="/reveal.js/dist/reset.css">
  <link rel="stylesheet" href="/reveal.js/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/reveal.js/dist/theme/black.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Understanding word2vec</h1>
  <p class="subtitle">Machine Learning for Dummies</p>
  <p class="author">Arnaud Bailly</p>
  <p class="date">2016-09-05</p>
</section>

<section>
<section id="motivation" class="title-slide slide level1">
<h1>Motivation</h1>

</section>
<section id="caveat-emptor" class="slide level2">
<h2>Caveat Emptor</h2>
<blockquote>
<p>All humans have equal intelligence;</p>
<p>Every human has received from God the faculty of being able to
instruct himself;</p>
<p>We can teach what we don’t know;</p>
<p>Everything is in everything.</p>
<p>Joseph Jacotot (1770-1840)</p>
</blockquote>
</section>
<section id="it-all-started" class="slide level2">
<h2>It all started…</h2>
<p>as a silly coding challenge to apply for a job:</p>
<blockquote>
<p>Extract the top 400 articles from Arxiv corresponding to the query
<code>big data</code>, analyze their content using Google’s word2vec
algorithm, then run a principal component analysis over the resulting
words matrix and display the 100 most frequent words’ position on a 2D
figure. In Haskell…</p>
</blockquote>
</section>
<section id="understanding-ml" class="slide level2">
<h2>Understanding ML</h2>
<ul>
<li class="fragment">Going beyond tools</li>
<li class="fragment">Going beyond (obscure) mathematical formulas and
theoretical principles</li>
<li class="fragment">Acquire intuitions about how ML works and can be
used</li>
</ul>
</section>
<section id="challenges" class="slide level2">
<h2>Challenges</h2>
<ul>
<li class="fragment">To understand how word2vec works</li>
<li class="fragment">To optimize word2vec for large data sets</li>
<li class="fragment">To complete a data analysis pipeline</li>
</ul>
</section></section>
<section>
<section id="understanding-basic-word2vec-algorithm"
class="title-slide slide level1">
<h1>Understanding Basic <code>word2vec</code> Algorithm</h1>

</section>
<section id="demo" class="slide level2">
<h2>Demo</h2>
</section>
<section id="principles" class="slide level2">
<h2>Principles</h2>
<ul>
<li class="fragment">Goal: Build a <em>words embedding</em> model,
e.g. a function <span class="math inline">\(e: W \rightarrow
R^d\)</span> that maps each word from a given vocabulary <span
class="math inline">\(W\)</span> to a high-dimensional <em>vector</em>
space</li>
<li class="fragment">Word2vec is actually more a <em>family</em> of
models:
<ul>
<li class="fragment">2 basic models: Continuous Bag-of-Words (CBOW) and
<strong>Skip-Gram</strong> and various optimisations</li>
<li class="fragment">Several variations</li>
</ul></li>
</ul>
</section>
<section id="principles-1" class="slide level2">
<h2>Principles</h2>
<p><img data-src="/images/w2v-model.png" /></p>
</section>
<section id="skip-gram-model" class="slide level2">
<h2>Skip-Gram Model</h2>
<p>Maximises probability of identifying context words for each word of
the vocabulary <span class="math inline">\(W\)</span></p>
<p><span class="math display">\[
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c\leq j \leq c, j\neq 0} \log
p(w_{t+j}|w_t)
\]</span></p>
<ul>
<li class="fragment"><span class="math inline">\(T\)</span> is the size
of the vocabulary <span class="math inline">\(W\)</span>, <span
class="math inline">\(w_j\)</span> is the <span
class="math inline">\(j\)</span>-th word of <span
class="math inline">\(W\)</span></li>
<li class="fragment"><span class="math inline">\(c\)</span> is the size
of the <em>context window</em></li>
</ul>
</section>
<section id="skip-gram-model-1" class="slide level2">
<h2>Skip-Gram Model</h2>
<p>Define conditional probability <span
class="math inline">\(p(w&#39;|w)\)</span> using <em>softmax</em>
function:</p>
<p><span class="math display">\[
p(w_O|w_I) = \frac{\exp(v&#39;_{w_O}^{\top} v_{w_I})}{\sum_{i=1}^{T}
\exp(v&#39;_{w_i}^{\top} v_{w_I})}
\]</span></p>
</section>
<section id="neural-network" class="slide level2">
<h2>Neural Network</h2>
<p><img data-src="/images/w2v-nn.png" /></p>
</section>
<section id="feed-forward" class="slide level2">
<h2>Feed Forward</h2>
<p><img data-src="/images/w2v-ff.png" /></p>
</section>
<section id="back-propagation" class="slide level2">
<h2>Back-Propagation</h2>
<p><img data-src="/images/w2v-backprop-output.png" /></p>
</section>
<section class="slide level2">

<p><img data-src="/images/w2v-backprop-input.png" /></p>
</section>
<section class="slide level2">

<p><span class="math display">\[
W_{new}&#39; = W&#39; - \alpha G_O
\]</span></p>
<p><span class="math display">\[
w_I_{new} = w_I - \alpha h&#39;
\]</span></p>
</section>
<section id="naive-code-in-haskell" class="slide level2">
<h2>(Naive) Code in Haskell</h2>
</section>
<section id="visualizing-word2vec-with-wevi" class="slide level2">
<h2>Visualizing word2vec with wevi</h2>
</section></section>
<section>
<section id="optimizing" class="title-slide slide level1">
<h1>Optimizing</h1>

</section>
<section id="problem" class="slide level2">
<h2>Problem</h2>
<ul>
<li class="fragment">While correct and straightforward, complexity of
basic implementation is huge: For each sample, we need to compute error
gradient over <span class="math inline">\(W&#39;\)</span> which has size
<span class="math inline">\(T x D\)</span>.</li>
<li class="fragment">Training speed is about <span
class="math inline">\(1/20^{th}\)</span> of reference
implementation</li>
<li class="fragment">Major contribution of word2vec papers is their
ability to handle billions of words…</li>
<li class="fragment">How can they do it?</li>
</ul>
</section>
<section id="proposed-optimisations" class="slide level2">
<h2>Proposed Optimisations</h2>
<ul>
<li class="fragment">Input words sub-sampling: Randomly discard frequent
words while keeping relative frequencies identical</li>
<li class="fragment">Parallelize training</li>
<li class="fragment">Negative sampling</li>
<li class="fragment"><strong>Hierarchical Softmax</strong></li>
</ul>
</section>
<section id="hierarchical-softmax" class="slide level2">
<h2>Hierarchical Softmax</h2>
<p><strong>Idea</strong>: Approximate probability over <span
class="math inline">\(V\)</span> with probabilities over <em>binary
encoding</em> of <span class="math inline">\(V\)</span></p>
<ul>
<li class="fragment">Output vectors encode a word’s <em>path</em> within
the binary tree</li>
<li class="fragment">Reduces complexity of model training to updating
<span class="math inline">\(\log(V)\)</span> output vectors instead of
<span class="math inline">\(V\)</span></li>
</ul>
</section>
<section id="huffman-tree" class="slide level2">
<h2>Huffman Tree</h2>
<p><img data-src="/images/w2v-huffman.png" /></p>
</section>
<section id="huffman-tree-1" class="slide level2">
<h2>Huffman Tree</h2>
<ul>
<li class="fragment">Huffman coding encode words according to their
frequencies: More frequent words are assigned shorter codes</li>
<li class="fragment">To each word is assigned a path in the Huffman tree
which tells, for each node, whether to go left or right</li>
<li class="fragment">Each node is assigned a row in the output
matrix</li>
</ul>
</section>
<section id="original-code" class="slide level2">
<h2>Original Code</h2>
<p><img data-src="/images/w2v-code.png" /></p>
</section>
<section id="less-naive-haskell-code" class="slide level2">
<h2>Less naive Haskell Code</h2>
</section></section>
<section>
<section id="more-challenges" class="title-slide slide level1">
<h1>More Challenges</h1>

</section>
<section id="functional-programming" class="slide level2">
<h2>Functional Programming</h2>
<ul>
<li class="fragment">Haskell is a pure lazy functional programming</li>
<li class="fragment">Data is immutable which leads to inefficiencies
when modifying very large data structures <em>naïvely</em></li>
<li class="fragment">Need to use mutable data structures and
<em>impure</em> code</li>
</ul>
</section>
<section id="data-acquisition" class="slide level2">
<h2>Data Acquisition</h2>
<ul>
<li class="fragment">Retrieve PDFs from Arxiv site</li>
<li class="fragment">Extract textual from PDF</li>
<li class="fragment">Cleanup text for analysis</li>
</ul>
</section>
<section id="data-visualisation" class="slide level2">
<h2>Data Visualisation</h2>
<p><img
data-src="https://www.tensorflow.org/versions/r0.10/images/linear-relationships.png" /></p>
</section>
<section class="slide level2">

<ul>
<li class="fragment">Find some way to reduce dimensionality of
space</li>
<li class="fragment">Most well-known technique is <strong>Principal
Component Analysis</strong></li>
<li class="fragment">Other techniques: t-SNE</li>
</ul>
</section>
<section id="computing-pca" class="slide level2">
<h2>Computing PCA</h2>
<ul>
<li class="fragment">Standard tool from statistical analysis and linear
algebra</li>
<li class="fragment">Transform the “basis” of the vector space to order
then by decreasing amount of variance</li>
<li class="fragment">Select the first 2 or 3 axis to display data on 2D
or 3D diagram</li>
</ul>
</section>
<section id="optimizing-pca" class="slide level2">
<h2>Optimizing PCA</h2>
<p><img data-src="/images/w2v-pca-perf-bench.png" /></p>
</section>
<section class="slide level2">

<ul>
<li class="fragment">Textbook computation of PCA means computing full
covariance matrix for dataset then eigenvectors of this covariance
matrix</li>
<li class="fragment">For a 50000 x 200 matrix this is extremely time
consuming…</li>
<li class="fragment">There exist iterative methods to compute principal
components one at a time</li>
</ul>
</section></section>
<section>
<section id="conclusions" class="title-slide slide level1">
<h1>Conclusions</h1>

</section>
<section id="machine-learning-is-hard" class="slide level2">
<h2>Machine Learning is Hard</h2>
<ul>
<li class="fragment"><em>In theory</em>: Get some data, find a suitable
model, fit model to data using standard logistic regression, use
model</li>
<li class="fragment"><em>In practice</em>:
<ul>
<li class="fragment">Datasets need to be large which means algorithms
need to be efficient</li>
<li class="fragment">Efficient algorithms require clever optimisations
that are non obvious</li>
<li class="fragment">Hard to get “right”…</li>
</ul></li>
</ul>
</section>
<section id="machine-learning-is-hard-2" class="slide level2">
<h2>Machine Learning is Hard (2)</h2>
<ul>
<li class="fragment">Still an active research field: Going from research
paper to tool is not straightforward</li>
<li class="fragment">Reverse engineering code was a painful process</li>
<li class="fragment">To really understand what one’s doing requires
understanding large chunks of maths: Linear algebra, statistics,
numerical analysis, Natural Language Processing…</li>
</ul>
</section>
<section id="machine-learning-is-fun" class="slide level2">
<h2>Machine Learning is Fun</h2>
<ul>
<li class="fragment">Stretches your programming skills beyond their
limits</li>
<li class="fragment">Forces you to tackle new concepts, techniques and
algorithms</li>
<li class="fragment">Expands knowledge base to cutting edge
technology</li>
<li class="fragment">Increases his/her love for you</li>
</ul>
</section>
<section id="takeaways" class="slide level2">
<h2>Takeaways</h2>
<ul>
<li class="fragment">There is no better way to understand algorithms
than to implement them</li>
<li class="fragment">For production, don’t roll your own ML engine
unless:
<ul>
<li class="fragment">that’s your core skills domain</li>
<li class="fragment">and/or you are prepared to spend time and
money</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level2">
<h2>References</h2>
<ul>
<li class="fragment"><a
href="http://arxiv.org/pdf/1301.3781.pdf">Original word2vec
paper</a></li>
<li class="fragment">Word2vec implementations: <a
href="https://github.com/dav/word2vec">original C version</a>, <a
href="https://radimrehurek.com/gensim/models/word2vec.html">gensim</a>,
<a
href="https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html">Google’s
TensorFlow</a>, <a
href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec">spark-mllib</a>,
<a href="https://github.com/medallia/Word2VecJava">Java</a>…</li>
<li class="fragment"><a
href="https://github.com/ronxin/wevi">Visualizing word2vec</a> and <a
href="http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf">word2vec
Parameter Learning Explained</a></li>
<li class="fragment"><a
href="http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/">Implementing
word2vec in Python</a></li>
<li class="fragment">Word2vec in Java as part of <a
href="http://deeplearning4j.org/word2vec#just">deeplearning4j</a>
(although word2vec is <strong>NOT</strong> deep learning…)</li>
<li class="fragment"><a
href="http://rare-technologies.com/making-sense-of-word2vec/">Making
sense of word2vec</a></li>
<li class="fragment"><a
href="http://arxiv.org/pdf/1402.3722v1.pdf">word2vec Explained</a></li>
<li class="fragment"><a
href="https://github.com/abailly/hs-word2vec">word2vec in
Haskell</a></li>
</ul>
</section></section>
<section id="questions-feedback" class="title-slide slide level1">
<h1>Questions &amp; Feedback</h1>

</section>
    </div>
  </div>

  <script src="/reveal.js/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="/reveal.js/plugin/notes/notes.js"></script>
  <script src="/reveal.js/plugin/search/search.js"></script>
  <script src="/reveal.js/plugin/zoom/zoom.js"></script>
  <script src="/reveal.js/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
