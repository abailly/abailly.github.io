---
title: Notes sur "Les décisions absurdes"
author: Arnaud Bailly 
date: 2013-12-31
---

Qu'y-a-t'il de commun entre l'explosion de la navette *Challenger*, la collision entre deux pétroliers dans le Golfe Persique, la
transformation d'une université d'entreprise en institut de formation externe, la persistance de transparents surchargés et
illisibles dans les communications d'entreprise, le pont de la rivière Kwai ? Toutes ces petites et grandes catastrophes, et bien
d'autres, sont le résultat de **décisions absurdes**, des décisions produisant de manière persistante des effets contraires au but
initialement recherché, analysées par [Chistian Morel](http://christian.morel5.perso.sfr.fr) dans le [livre portant ce titre](http://www.gallimard.fr/Catalogue/GALLIMARD/Folio/Folio-essais/Les-decisions-absurdes).

L'intérêt principal de ce livre, en plus de son style agréablement didactique et subtilement ironique, est de ne pas se contenter
d'identifier une cause unique pour toutes ces cas d'erreurs extrêmement variés mais de couvrir un ensemble de causes potentielles
qui, se combinant, sont à même de produire ces comportements absurdes observables chez des individus et dans des organisations
ayant par ailleurs atteint un très haut degré de sophistication et de scientificité. C'est donc à la conjugaison d'erreurs
*cognitives*, de *comportements collectifs* aberrants et de défaillances *téléologiques* que l'on doit s'intéresser pour mieux
comprendre, et peut-être éviter, les décisions absurdes.

Autrement dit, il n'y a jamais de responsabilité unique, individuelle ou attribuable à un petit groupe, pour ces erreurs, mais
toujours une dérive d'un système rationnel, comme un *cancer de la raison* qui petit à petit viendrait substituer des métastases
aux cellules saines d'un organisme. Le cas de *Challenger* est emblématique et cumule à un haut degré les trois types d'erreurs
pour aboutir à une décision catastrophique:

* les ingénieurs et managers de Morton-Thiokol commettent d'abord des erreurs *cognitives*, d'une part au sujet du climat hivernal de la
  Floride, transposant une faible probabilité de grand froid en une impossibilité, d'autre part au sujet de la fiabilité des
  joints lors des lancements. Ce phénomène a été décortiqué en détail par Kahneman et al. et
  [Thinking, Fast and Slow](http://us.macmillan.com/thinkingfastandslow/DanielKahneman) est une introduction pour le commun des
  mortels au modèle cognitif proposé par Kahneman et reprit partiellement par Morel, celui des deux systèmes : le *Système I* ou
  heuristique, efficace et rapide mais imprécis et prompt à se tromper et le *Système II* ou analytiau
* le processus au cours duquel est prise la décision de maintenir le lancement renforce ces erreurs cognitives pour produire un
  consensus mou. La décision finale est prise au cours d'une conférence téléphonique entre trois lieux différents des
  États-Unis, les doutes de certains ingénieurs ne sont pas perçus comme déterminants par les *managers* d'autant plus qu'ils ne
  parviennent pas à les formuler scientifiquement, le jeu des relations de pouvoirs affaiblit les signaux négatifs...
* le fruit est alors mûr pour que la troisième source d'erreurs, la perte de sens ou erreur téléologique, entre en jeu. Les
  participants à la décision perdent de vue la fin - faire voler en sécurité 7 astronautes dans une navette spatiale - au
  profit des moyens - les joints des *boosters* sont-ils dangereux et Morton-Thiokol est-il uin fournisseur fiable ? La
  préservation du système prend le pas sur la finalité pour laquelle ce système a été construit.

En conclusion du livre, l'auteur remarque par ailleurs que les erreurs sont liées à *l'indétermination fondamentale du
monde*. Ce qui rend la vie excitante et les décisions nécessaires est aussi ce qui rend ces décisions positives ou négatives, ce
qui provoque le succès ou échec, l'erreurs ou la bonne décision. De
très nombreuses situations sont sous-déterminées, nous n'avons pas les réponses toutes prêtes et, particulièrement en situation
de stress et d'urgence, il nous faut savoir mobiliser à la fois notre Système I, heuristique et expérientiel, et notre Système II,
analytique et scientifique.

Les organisations habituées à l'urgence, au danger et au stress - armée, secours civil, contrôle
aérien, chirurgie, médecine d'urgence - développent des techniques similaires pour que leurs membres soient capables de faire face
à ces situations par définition imprévisibles:

* s'entraîner de manière systématique, à l'aide de protocoles précis, développés par capitalisation des expériences, développer
  des réflexes de *survie* mobilisables instantanément afin de libérer le cerveau pour d'autres tâches ;
* développer l'autonomie et la cohésion des équipes par la compréhension et l'incorparation des *fins* et des *buts* de
  l'organisationm afin que les décisions prises lors de "situations critiques" aient le plus de chances d'être adaptées à la
  situation et non polluées par des considérations extérieures, erreurs cognitives, systémiques ou téléologiques.

Même si au sein d'une équipe de développement il est très rare de vivre des situations aussi dramatiques, nous commettons beaucoup
d'erreurs qui se trouvent reflétées dans notre code sous la forme de *bugs* , manifestes ou latents - la fameuse *dette
technique*. Mais comprendre les sources de ces erreurs, et savoir les combattre, est une nécessité de plus en plus impérieuse dans
un monde qui va de plus en plus vite et qui est en train d'être [mangé par le logiciel](https://gist.github.com/sferik/5277512).

J'attends avec impatience la parution du tome II en poche intitulé [Les décisions absurdes II. Comment les éviter](http://christian.morel5.perso.sfr.fr).
