<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Arnaud Bailly's  Blog</title>
    <link href="http://abailly.github.io/atom.xml" rel="self" />
    <link href="http://abailly.github.io" />
    <id>http://abailly.github.io/atom.xml</id>
    <author>
        <name>Arnaud Bailly</name>
        <email>arnaud@igitur.io</email>
    </author>
    <updated>2016-05-05T00:00:00Z</updated>
    <entry>
    <title>Sur &quot;Critique de la raison pure&quot;</title>
    <link href="http://abailly.github.io/posts/kant.html" />
    <id>http://abailly.github.io/posts/kant.html</id>
    <published>2016-05-05T00:00:00Z</published>
    <updated>2016-05-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>Sur &quot;Critique de la raison pure&quot;</h1>
<div class="info">Posted on May  5, 2016</div>

<p>Ceci est une retranscription d’un cours improvisé par <a href="https://directory.unamur.be/staff/pavez">Peggy Penez-Avez</a> lors d’une session d’<a href="http://agenda-agile.org/showevent.php?id=487">Agile Open France 2016</a>, conçu plus ou moins comme une expérimentation du <a href="http://dojophilo.net/">Dojo Philo</a>. J’ai pu ajouté quelques notes personnelles…</p>
<ul>
<li>Kant développe une <em>philosophie transcendantal</em> et cherche à répondre à la question <em>“que puis-je connaître ?”</em>, qui peut se comprendre à la fois comme <em>“qu’est ce qui est digne d’être connu ?”</em> et <em>“qu’est ce que mes facultés me permettent de connaître ?”</em></li>
<li>Il s’agit donc d’explorer les condition de possibilité d’une connaissance, qui se résument (à l’époque de Kant) à trois approches:
<ul>
<li><strong>dogmatique</strong>: on peut tout connaître et en particulier l’existence de Dieu peut être objet d’une connaissance, toute la métaphysique est un objet de connaissance. Le problème, c’est que les thèses métaphysiques sont en désaccord les unes avec les autres, qu’il semble ne pas y avoir de progrès dans ce domaine : peut on vraiment parler de connaissance alors ?</li>
<li><strong>sceptique et empiriste</strong>: Hume réveille Kant du sommeil de la raison. Pour les sceptiques, la base des idées ce sont les observations et l’expérience, pas la raison (interne) : je dis que le soleil va se lever parce que j’en ai l’habitude, qu’il se lève tous les matins que j’ai pu expérimenter. L’esprit a l’habitude de voir une succession entre les phénomènes et en déduit une relation de <em>causalité</em> mais rien ne permet de prouver que c’est bien la réalité (en termes modernes, on pourrait parler d’une confusion entre <em>corrélation</em> et <em>causation</em>). Ce qu’on prend pour de la connaissance n’est que de la <em>croyance partagée</em></li>
<li><strong>critique</strong>: l’expérience est fondamentale pour qu’il y ait connaissance, mais pas que. C’est l’approche de Kant.</li>
</ul></li>
<li>Les questions de l’existence de Dieu, de l’âme et d’autres questions métaphysiques ne peuvent pas être objets d’expérience. Ces idées ne sont pas dans la connaissance mais dans la <em>pensée</em> et il y a donc une différence entre pensée et connaissance</li>
<li>Il faut donc exclure du champ de la connaissance les objets de la philosophie et en faire donc la <em>critique</em></li>
<li>Ce que nous connaissons par l’expérience, nous le connaissons <em>objectivement</em>, donc il n’y a pas d’expérience s’il n’y a pas de <em>catégories</em> en nous permettant de construire cette connaissance à partir de l’expérience</li>
<li>Le <em>transcendental</em> est ce dont l’expérience a besoin pour exister, ce qui est <em>a priori</em> et permet de former des jugements synthétiques a priori
<ul>
<li>Les formes a priori de la sensibilité (intuition) sont le <em>temps</em> et l’<em>espace</em> : tout ce que je perçois est dans l’espace et le temps, c’est une disposition qui est nécessairement en nous pour qu’il y ait expérience</li>
<li>Les concepts a priori de l’entendement sont au nombre de 12, par groupes de 3 (inspirés des catégories aristotélicienne):
<ul>
<li>Quantité: Unité, Pluralité, Totalité</li>
<li>Qualité: Réalité, Négation, Limitation</li>
<li>Relation: Substance/accident,Cause/Effet,Réciprocité</li>
<li>Modalité: Possibilité/impossibilité, Existence/non-existence, Nécessité/contingence</li>
</ul></li>
</ul></li>
<li>Les objets d’expérience possibles sont les <em>phénomènes</em>. Rien ne prouve que le réel soit identique aux phénomènes, le réel <em>en soi</em> (nouméne) ne peut être connu.</li>
<li>L’enjeu de Kant qui s’inscrit dans le mouvement des Lumières est d’atteindre à l’universalité de la raison. Ce que l’on peut connaître et penser peut l’être par tous les êtres humains, parce qu’il ne peut être autrement que ce qu’il est en vertu des conditions nécessaires à la raison</li>
<li>La méthode relève quelque peu de l’imagination d’un maniaque qui prétendrait repartir de 0. Kant divise l’exposition en:</li>
<li>l’esthétique transcendantale, qui est la définition des formes de la sensibilité</li>
<li>l’analytique transcendantale, qui construit les concepts a priori de l’entendement</li>
<li>la dialectique transcendantale enfin, qui définit les limites de la connaissance et ce que l’on peut penser en dehors d’elle.</li>
<li>Si je ne peux pas connaître Dieu, pourquoi en ai-je l’idée ? C’est un travers de l’esprit qui cherche systématiquement à absolutiser : par exemple l’idée de Dieu est l’absolutisation de la <em>causalité</em></li>
<li>Cette tendance à la totalisation nous pousse à l’inconditionné, à chercher des fondements absolus et définitifs à toute chose</li>
<li>C’est une “maladie” de l’esprit d’élaborer des illusions à partir des catégories de l’entendement, en dehors de toute expérience possible</li>
<li>Mais par ailleurs l’impossibilité de s’arrêter à l’expérience fait de l’homme un être libre et moral. Les idées de la raison qui ne sont pas de l’ordre de la connaissance sont la condition de la vie morale</li>
<li>L’idée morale chez Kant consiste à renverser la proposition “tu peux donc tu dois” en “tu dois donc tu peux”: c’est l’impératif catégorique</li>
</ul>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>On &quot;The World Beyond Your Head&quot;</title>
    <link href="http://abailly.github.io/posts/the-world-beyond-your-head.html" />
    <id>http://abailly.github.io/posts/the-world-beyond-your-head.html</id>
    <published>2016-04-28T00:00:00Z</published>
    <updated>2016-04-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>On &quot;The World Beyond Your Head&quot;</h1>
<div class="info">Posted on April 28, 2016</div>

<p>I have read Matthew Crawford’s <a href="http://www.editionsladecouverte.fr/catalogue/index-__loge_du_carburateur-9782707160065.html">Éloge du carburateur</a> or <a href="http://www.matthewbcrawford.com/new-page-1-1-2/">Shop Class as Soulcraft</a> a few years ago, in French and really liked the way it praised manual work, of the kind one does in a mechanics shop, and reconciled it with intellectual work. I bought <a href="http://www.matthewbcrawford.com/new-page-1-1/">The World Beyond Your Head</a> and read it in English this time.</p>
<p>I have mixed feelings about Crawford’s statements and philosophy and I am writing this post to make those feelings clearer to my self, to try to separate what I like and what I don’t like, and possibly to raise discussion and criticism. After all English is not my mother tongue and I am not a professionnal philosopher hence there are things I might have misunderstood.</p>
<h1 id="summary">Summary</h1>
<p>The book’s subtitle - <em>How to Flourish in an Age of Distraction</em> - exposes its program: A moralist view on our (post-)modern condition entrapped in the thralls of <em>engineered hyper pallatable desires</em> bestowed upon us by big business, and how to escape this condition and rebuild meaning.</p>
<p>The book starts with the somewhat obvious statement that our <em>attention</em> has becomed a key resource that is being harvested by companies and governments. The former seek to extract profit by engineering experiences that will attract and retain our attention, as exemplified by how successful Internet giants are gathering personal data and constantly creating new ways of making profit out of that (big) data. The latter seek enhance social control and <em>conformance</em> in order to govern through consensus and norm, rather than through strength and police. This is exemplified by <em>nudges</em> which are now routinely used to induce expected behaviour without resorting to coercition.</p>
<p>The question of <em>attention</em> is key as it is the expression of our relationship to the world and ultimately to our individuality, or coherence of <em>self</em>. The <em>homo economicus</em> view of free will and rationality that is our common horizon those days implies that we are individually responsible for our attention and exercise of free will, that we can <em>rationally choose</em> at all time what to focus on. But advances in psychology and behavioural economics tend to demonstrate this is not actually true, that our desires and thoughts are driven by our culture and environment, leading to a deterministic view of human rationality and ultimately to engineering our behaviour, at least statiscally.</p>
<p>Through the example of the <em>motorcycle rider</em>’s experience, the author offers another view of attention and free will, one that relates the mind to <em>reality</em> through <em>affordances</em>, or rather one that makes the individual situated, a part of, embodied, in reality. By dismissing the view of the mind - or self - as an <em>observer</em> of reality, one can account for the rider’s experience which would seem impossible to explain in purely rational terms. The experience of the road and riding one accumulates through direct relationship with the reality, the speed of wind, the road, the noise of the engine, the surrounding traffic must be physical, mediated through the motorcycle, makes riding possible. And this cannot be engineered but must be learnt: The increasing sophistication of today’s cars remove the driver from the experience of driving thus removing the possibility of taking decisions, leading ultimately to the driverless car.</p>
<p>Crawford’s give two counterexamples to this <em>situated self experience</em>. The first one is the <em>Mickey Mouse Clubhouse</em> which is a TV program with educational motivation but which depicts a highly virtual and <em>magical</em> world: Problems happening to Mickey and his friends are almost invariably solved by pushing some button and invoking some magical device that solves the problem. The second example is how gambling industry crafts the gamblers’ environment to induce more and more gambling and generate addiction. The goal of the whole gambling industry is to make gamblers lose contact with reality so that gambling <em>becomes</em> reality.</p>
<p>The extreme libertarianism that expresses itself in those few examples and the accepted social norm of individualism and self-responsibility of adults that makes all this possible can be traced back, according to Matthew Crawford, to the philosophers of the European Enlightnement, Descartes, Locke and last but not least Kant. Those philosophers struggled against the absolutism of their time and the dogmatism of the Church and King, and they built a philosophy based on the overarching principle of a fully autonomous self, radically separated from the World. Kant’s moral philosophy in particular, needs to posit one’s moral self as disconnected from any particular experience in order to be able to reach universalism. The <em>moral imperative</em> that one shall only treat others as an end and never as a mean is universal because it is <em>not</em> inferred from experience or intuition but is at the root of our judgment. And Locke’s rejection of authority to make individual freedom possible implies a radical shift on the source of truth from the outside - authorities, dogma - to the inside - one’s mind and rational judgment. In the end, we are led to live in a world of <em>representations</em> that abstract ourselves from the reality.</p>
<p>The second part of the book offers another view of individualism as <em>shared experience</em> of the world. From the baby which is led to discover the world by her caregiver to the student or apprentice which is led to discover a new field and learn new skills by a teacher or master, one can infer that our individuality <em>needs</em> other people and things, needs an actual - dialectical as Hegel says - confrontation with the reality <em>beyond my head</em>. But this requires <em>attention</em>, and <em>care</em>, and <em>effort</em> as the real world might not be very nice or my be adversarial to my current desires and feelings. Yet because there is a feedback loop between the inner self and the outer world, one can choose to consciously act in order to change his feelings and thoughts. Crawford talks about the <em>erotics of attention</em> or the desire to encounter an <em>other</em> as a way to enrichen and enliven my own <em>self</em>.</p>
<p>But a key contradiction of our modern world is that although it theoretically puts individualism as the ultimate measure of things, in practice we are more and more treated as <em>statistical units</em>, e.g. generic representatives of some group, trend, fashion, community. This movement started in the 20s and 30s with the advent of polls, going hand in hand with industrialisation and rationalisation of organisations and of course the triumph of collectivist movements. But Tocqueville wrote about it a century ago when he described the <em>tyranny of majority</em> in democratic America. Enjoined on one side to <em>be oneself</em> and on the other side pressured to <em>abide by the social norms</em>, the modern person seeks refuge in political correctness, fashion and muzak, keeping one’s own thoughts private lest he or she runs the risk of being confronted to others. Kant is once again convicted of being at the root of this contradiction in his efforts to build a universal moral: Each man or woman must act as being a representative of humanity as a whole and som ust somehow lose his or her individuality in the process. As Matrix said it: <em>Welcome in the desert of the real</em>.</p>
<p>The third and shortest part of the book offers a path out of this modern <em>flattening</em>, and this path builds on <em>inheritance</em> and <em>craftsmanship</em>. The author visits <a href="http://www.taylorandboody.com/">Taylor and Boody</a>, a small organmaker shop in the U.S., and through what he experiences there and what the people working there tell about their work, suggests that organmaking is a good archetype of how to rebuild true individuality in our modern days. The key characteristics of the organmakers is that:</p>
<ol style="list-style-type: decimal">
<li>They are inheritors to a centuries old tradition, building upon techniques, tools and artefacts that were devised in the 17th century or 18th century,</li>
<li>They are not lavishly <em>imitating</em> the past, they are working in constant <em>discussion</em> with the old masters, sometimes replacing outdated materials with modern ones (e.g. carbon fiber), sometimes rediscovering lost techniques. There actually was a Baroque organ revival in the first half of the 20th century that lead to renewal of techniques to uncover lost sound of organs,</li>
<li>They are <em>situated</em> and <em>embodied</em> in reality, crafting complex objects out of carefully chosen materials, using skillful techniques that require years to master,</li>
<li>They are working as a community, led by the founders but running in a somewhat self-organised process where each person knows what she has to do and each one has his or herown area of expertise. Because Baroque organs are complex and very expensive things to build, customers understand the constraints and in particular that one cannot build an organ in days,</li>
<li>They are building tools that are supposed to last for centuries, just like the organs from previous periods they are repairing or studying.</li>
</ol>
<p>Matthew Crawford concludes his book with a call to <em>reclaim the real</em> and in particular to overhaul our education system which is currently <em>undeserving of the attention of the students</em>. This overhauling should go through a rehabilitation of hand-crafting, giving the possibility to students to reclaim the real by actually working on it.</p>
<h1 id="discussion">Discussion</h1>
<p>I find the French title of the book <em>Contact</em> better than the English one: It better conveys the idea this book is about getting back in touch with the real and reclaiming it for ourself, to live it fully and consciously and not mediated through experiences crafted by others for their own means.</p>
<p>As a software developer, this is something I have sometimes been struggling with. Because we work mostly in a virtual world, and a very abstract one where we are required to manipulate concepts interacting in very complex ways, it is a constant temptation to isolate oneself from the material reality. In a sense we as developers play a key role in the <em>attention engineering</em> movement Crawford is criticizing as it is through our tools, techniques, software, data centers, algorithms that people from Big Corps can harvest so much personal data and manipulate our interaction with the world to suit their needs.</p>
<p>Social networks, virtual reality, hyper-communication, apps… all struggle to interpose themselves between us and reality in order to monetize those interactions, directly or indirectly. They all strive to capture a share of our attention and to impel their representations on us. They are mediating our access to the real and like all media before them, because they mediate this access to the world, they can control and tweak it. Or they can use the gathered data to serve other needs like advertising, industry optimisation or mass surveillance. And as software developers we have a responsibility with this state of the world, we cannot dismiss the issue saying <em>we are just techs</em>.</p>
<p>Yet those tools we are creating and using are also giving us access to another reality. They make new <em>interactions</em> possible that were not even dreamt of in the past. It is possible to focus one’s attention on things that matter in virtual worlds, or in social networks. It is possible to foster and nurture deep - even <em>erotic</em> - interactions with people living on the other side of the planet. I have some personal experiences pair programming with people remotely that were really rich, and I have build friendship through virtual communications before I have a chance to meet people in real life. I believe, from first-hand experience, that the kind of individuality which is appealing to Crawford, individuality built on shared experience and confrontation can be built through mediation. But this of course requires energy, and actually more energy than one would need in real life, as the signal vs. noise ratio is lower hence misunderstanding can happen more quickly. And this also requires <em>action</em> and <em>intention</em>, the reliance on more neutral or individually-controlled media, e.g. open-source, free software, community run servers and networks…</p>
<p>I also felt much empathy with Crawford’s praise of craft and the account he gives of Taylor and Boody’s way of working. As a long-time proponent of agile development, self organising teams, software craftsmanship and software quality, extreme programming… I cannot but recognise the way I would like to work, and sometimes managed to work, in how he describes teamwork at the organmakers’ shop. In the narrowest sense, software development obviously rests on a much younger tradition than organmaking. Yet even with a decades old tradition it is possible to build the kind of critical dialogue Crawford highlights in Taylor and Boody’s work:</p>
<ul>
<li>More often than not we are working with existing software and systems. This so-called <em>legacy code</em> is often viewed negatively, carrying with it stigmatas of impenetrability, accidental complexity, technological obsolescence… Yet this code has often been around for years and served its purpose well, and it is actually running the business be it a for-profit company or a non-profit organisation. It may have more than its share of warts but this is where the capability to engage in a <em>critical dialogue</em> comes into play. One of my favourite software development books is still Michael Feather’s <a href="">Working Effectively with Legacy Code</a>,</li>
<li>Furthermore, we have built over the past 50 or 60 years a body of knowledge and experience which is still valuable, and valued. Technology fads come and go, but computer science, algorithms and engineering principles stay and accumulate. And with these too we can engage in a critical dialogue. <a href="https://en.wikipedia.org/wiki/Dijkstra&#39;s_algorithm">Dijkstra’s shortest path</a>, <a href="http://lcm.csa.iisc.ernet.in/dsa/node184.html">Kruskal’s spanning tree</a>, <a href="http://www-formal.stanford.edu/jmc/recursive.ps">McCarthy’s LISP</a>, <a href="http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf">Church’s lambda-calculus</a>, <a href="http://spinroot.com/courses/summer/Papers/hoare_1978.pdf">Hoare’s CSP</a>, <a href="https://intuitionistic.files.wordpress.com/2010/07/martin-lof-tt.pdf">Martin-Löf’s type theory</a>, <a href="http://infolab.stanford.edu/pub/papers/google.pdf">Page and Brin’s page rank</a> are all examples of knowledge that exist we must take care of when building new software. There is a kind of breathing in computer systems going back and forth between centralised systems with thin clients and decentralised systems with fat clients. Work on shared memory and parallel computers from the 80’s and 90’s is relevant for today’s massively distributed systems. And I am not even talking about the huge body of knowledge from mathematics, social sciences, physics, psychology one could put to good use when developing software systems.</li>
</ul>
<p>Moreover it is not true that our trade is a purely intellectual one. When working on a difficult problem or when encountering some unexpected road-block that forces me halt, it is often the case I grow a pain in my back, like I have been <em>pushing</em> a big rock uphill or doing some hard workout. Thinking can be helped or hindered by our physical sensations, the way we move or put to rest our body, walking, sitting, couching. Going out for a walk helps solving tough problems just like going to sleep while letting problems to rest can often have a “magical” effect. Drawing, writing or even typing are all part of our thinking process, or at least part of my thinking process. <a href="/posts/cake-custard-category.html">Eugenia Cheng</a> is not only a great category theorist, a pure mind, she is also a concert-level pianist. And how many great programmers do I know who also practice some sport or art, sometimes at competitive level?</p>
<p>More generally, it is obvious the brain needs fuel and energy to work at its best and being well fed, rested, energized with a healthy way of life has deep impact on how fast, deep and well we can think. We <em>are</em> bodies and any attempt at denying this fact leads nowhere.</p>
<p>Interestingly, Kant himself, whom Matthew Crawford criticizes so much in his book as being at the root of all the modern evil induced by individualism, the paragon of <em>idealism</em> nevertheless relates directly mind to body. In his <a href="https://en.wikipedia.org/wiki/Critique_of_Pure_Reason">Critics of Pure Reason</a> Kant aims at providing a grand unifying theory of <em>knowledge</em>. From what I understand - feel free to correct me - Kant posits that <em>knowledge</em> comes from the interaction of <em>understanding</em> which is the part of our mind that forms concepts and <em>intuition</em> which is the part that forms experiences. Both faculties are made possible because there exists <em>a priori</em> categories that <em>shape</em> understanding and intuition: <em>Space</em> and <em>time</em> are the tow categories that shape our experience of the world, while <em>causality</em>, <em>totality</em>, <em>unicity</em> and a bunch of others are the categories that shape concepts. I won’t - and can’t - delve into the intricacies of Kant’s philosophy but from what I understand Kant does <em>not</em> state the subject or mind exists independently of the world: On the contrary, he explicitly states that knowledge is only derived from experience and refutes <em>subjective idealism</em>: Self-consciousness exists through interactions with the real world, even though we only perceive phenomena and not the <em>reality in itself</em>. Time and space are “just” the shapes our mind give to our experience.</p>
<p>Trying an analogy, I would say that Kant demonstrated or discovered the <em>language of the mind</em>, and he showed this language is separated in <em>pure</em> - the understanding and concepts - and <em>impure</em> - the intuition and senses - parts. The <em>pure</em> part is what is used to produce meaningful results, but it is useless and inobservable without the <em>impure</em> part which gives it <em>data</em> to process and means to act on the world. “What is the sound of a single hand clapping?” asks the famous Zen koan.</p>
<p>Moreover Kant’s main goal seems to have been to establish solid philosophical foundations for <em>universality</em>: Of objective knowledge, reason and moral judgments. How can different people, with different experiences, can ever reach agreement, know something in common, act for a common good, if there is nothing else but <em>subjective monads</em> isolated from one another? If experience is always personal, nothing can ever be shared. Kant’s answer lies in the <em>a priori</em> of the understanding and perception which shape the way we think and thus makes shared knowledge and perceptions possible, because we all share the same internal machinery to <em>interpret</em> the phenomenal world.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The ambivalent feelings I have with this book can be stated in a very simple way: Although I subscribe to the conclusion of the book, I am doubtful about its premises and I fear that he might be throwing the baby out with the bath’s water:</p>
<ul>
<li>There is a need to reclaim our attention which has been captured and privatised by corporations and state,</li>
<li>This should come from increasing our <em>surface of contact</em> with reality, something that is fostered by shop work and skillful knowledge,</li>
<li>The master-apprentice relationship and the transmission that happens there is key to build skillful and meaningful knowledge, one that is <em>situated</em> both in space and time,</li>
<li>However this <em>communautarian</em> and <em>aristocratic</em> process might come at the price of universality and democracy, as exemplified by the stratified and fragmented societies of the past against which philosophers of the Enlightenment struggled. <em>Corporatism</em> is the reverse of a coin whose observe is <em>craftsmanship</em>.</li>
</ul>
<p>I don’t follow Crawford in his rejections of the project of the <em>Enlightenment</em>, namely the project of founding a universal community of humans. Of course, in the time and place this project’s was born, humanity was restricted to wealthy white european males. And the time of the <em>great tales</em> has passed: We live in post-(post?-)modernist times where irony reigns. And Kant’s philosophy itself might be outdated. But I am not ready to throw away the idea that we need to found, and refound again and again, some form of universal that makes it possible to share the world’s experience and build common knowledge.</p>
<p>I cannot conclude this post without citing Spinoza whose philosophy traces the kind of path Matthew Crawford’s is proposing to us, one of reconciling mind, body and the world in a universal shared by all human beings. Spinoza viewed reality was made of a single <em>substance</em> which manifested itself through an infinity of <em>attributes</em>, only two of which are accessible to us: <em>thought</em> And <em>extension</em>. Hence the deep parallelism between the mind and the body: Whatever affects one affects the other. From those principles Spinoza builds a philosophy that seeks to enhance <em>life</em> maximilizing <em>joy</em> under the guidance of <em>reason</em>. Spinoza himself exemplified this unity of mind and body by being one of the most renowned lens polisher of his time.</p>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>Using agile in a startup</title>
    <link href="http://abailly.github.io/posts/agile-startup.html" />
    <id>http://abailly.github.io/posts/agile-startup.html</id>
    <published>2016-04-24T00:00:00Z</published>
    <updated>2016-04-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>Using agile in a startup</h1>
<div class="info">Posted on April 24, 2016</div>

<p>This post is a follow-up to my <a href="/posts/cm-arch-design.html">previous post</a> on designing an application in Haskell and part of a series of post I intend to write that reflect on my experience as CTO of Haskell-based startup. Whereas the previous post was very technical, focusing on <em>what</em> we built, this one focuses on <em>how</em> we built it: Processes, methods and tools we used, how the team was built and how it interacts on a daily basis, which problems we encountered and how we solved them (or not)… As before I have tried to be as honest as possible, highlighting both what worked and what did not work so well.</p>
<p>Coming from an Agile background it is no surprise that I consciously tried to reuse the knowledge and tools I knew from previous implementations of Scrum, eXtreme Programming or Kanban. One of the lesson learned during the past year is that Agile needs some serious overhauling to be applied within a startup setting, even more so when the team is distributed. In the conclusion I try to draw on this experience to hint at some trails I would like to explore in the future.</p>
<h1 id="a-bit-of-context">A Bit of Context</h1>
<p>I started working on Capital Match at the end of August 2014. The first commit in the git repository of the application is dated on Wednesday, 20th of August 2014. I worked in the background for a couple of months, mostly to setup the infrastructure, flex my muscles on Haskell, explore the business domain space.</p>
<p>I met physically my co-founder in October. At that time I was still working alone but I already had a working solution that was deployed and “usable”, for some definition of usable. Willem van den Ende joined me to develop the application to production stage at the end of October and brought with him lot of XP and DevOps experience. We were joined by another developer at the beginning of March but this employment lasted only a month.</p>
<p>We went live on the 20th of March, one month after the initial target set in December. The tech team is now 4 people, including me, all of which are mainly developers. We do not have a dedicated operations team and the tech team is responsible for both developing new features and maintaining the application in good operational conditions.</p>
<h1 id="planning">Planning</h1>
<h2 id="project-planning">Project Planning</h2>
<p>We started using <a href="http://taiga.io">taiga.io</a> from November to plan the go-live, first without then with spring planning. It was a good thing to explain to non-tech people we can deliver so much in a period. We quickly settled on 1 week sprints and it took us a while to get to some form of regularity and cadence, never really get to smooth release and meaningful <em>Story Points</em> values. A month or so after go-live we switched to a kanban style process which is not really well supported by taiga.</p>
<p>It seems people are having a hard time understanding SP, and when they understand they start trying to game it and discuss evaluations. It fairly quickly gets back to “when will it be done?” and “can’t we really do it faster?” and “what if we did XX?” questions… Smart people used to work with figures are prone to get to the conclusion figures are acutally measuring output and value and then we will use those figures to measure success.</p>
<p>We dropped formal estimates, even before golive: what’s done is done, when it is not done it is not done. In order to cope with unrealistic expectations and requirements we strived hard to make everything visible, deploy very quickly and allow people to test easily, work in baby steps… in other words reduce the length of the <em>feedback loop</em>. We don’t provide estimates anymore, and people don’t ask for them, except in very broad terms for high-level features which we (I) estimate as S/M/L or some combination thereof.</p>
<p>People also had a hard time using taiga (and I am not sure it would have been better with any other tool…): Response is a bit slow, most of the things don’t make sense for them, they don’t follow really tickets and comments on them… nor do they care much about tracking SPs progress.</p>
<p>So we finally switched to using plain trello:</p>
<ul>
<li>Its interface is very intuitive to pick, and most people already have used it at some previous point,</li>
<li>The user interface is super-fast and reactive,</li>
<li>It provides nearly all the features we need, except swimlanes but we use tags instead,</li>
<li>It is available on a wide variety of platforms, e.g. as a mobile app.</li>
</ul>
<p>We currently use 2 boards:</p>
<ul>
<li><em>Software Development</em> board is used for day-to-day work. It contains baby step features and bugs, and has very simple workflow:
<ul>
<li><em>To prioritize</em> is used for brain dumping features, and we try to clean up at frequent interval,</li>
<li><em>To Do</em> is for actual stuff to do. We strive to keep those items fine-grained, e.g. no more than 1-2 day of work,</li>
<li><em>Doing</em> says it all. We try to minimize Work-in-progress by ensuring people do not work in parallel on more than one feature. It is at this stage that tickets are assigned and not before,</li>
<li><em>Ready for testing</em> means ticket has passed CI (more on this later) and is ready to be tested on <em>staging</em> platform with non-production data. Once or twice a day we send an email notifying people of whatever is available to test on <em>staging</em> platform,</li>
<li><em>Deployed</em> means feature has been deployed to <em>production</em> platform. Just like for staging, we send an email to involved people every time we deploy new feature to production.</li>
</ul></li>
<li><em>Product roadmap</em> board is used for larger scale planning. It contains 3-months columns and coarse grained features estimated using S/M/L t-shirt sizes. There is one <em>Ongoing</em> column which contains features we are currently working on. It is mostly used as a reminder of “large” things we want/need to do.</li>
</ul>
<h2 id="product-development">Product Development</h2>
<p>I wrote a “walking skeleton” in a month or so, starting from initial idea, while my partner was sorting out the business side of things and trying to find seed investors in the project. This walking skeleton was, well, very skeletal, being able to handle only very basic scenario: register borrower/investor, create new simple, loan, commit funding to loan then accept loan.</p>
<p>When go live started to come close, business people got scared and wanted to go into <em>command and control</em> mode, and wanted to go into the following process:</p>
<ul>
<li>They designed very detailed “screens” in Excel with lot of complex business rules, strict wordings… covering the complete scope of the application as we initially discussed it,</li>
<li>Devs would work one screen at a time and <em>complete</em> it before going to the next,</li>
<li>Then they would tests what devs produce,</li>
<li>Then bugs would be notified and needed to be fixed,till business is happy</li>
<li>In short, we were reinventing <em>Waterfall</em> without the good parts (e.g. formal V&amp;V)</li>
</ul>
<p>I used the initial provided design documet to slice everything into small stories with estimates and put that into Taiga, then we started working on it following the aforementioned process. After 1-2 weeks it became obvious to everybody we would not be able to deliver everything that has been designed in excruciating details, because we simply could not code fast enough. Something like about 40-50% only of the designed features were actually shipped on go-live. Hence we reverted to priority-based scheduling and <a href="http://agileatlas.org/articles/item/dimensional-planning">Dimensional planning</a> with one week sprints and a target release date.</p>
<p>At some early stage I tried to use <a href="http://www.agileproductdesign.com/presentations/user_story_mapping/e">Story mapping</a>, maintaining a map of the software’s features using <a href="https://www.mindmup.com/">MindMup</a> a hosted mind-mapping application:</p>
<div class="figure">
<img src="/images/story-map.png" alt="A fragment of Capital Match’s Story Map" />
<p class="caption">A fragment of Capital Match’s Story Map</p>
</div>
<p>The idea was that we would represent the whole process in a map, distinguishing manual/automated parts and filling the leaves with actually available features. However this did not gel and people never get accustomed to use that representation of the software, so I quickly stopped maintaining it.</p>
<h1 id="programming-practices">Programming Practices</h1>
<h2 id="pair-programming">Pair Programming</h2>
<p>We started doing remote pairing with Willem and we still try to do it on a regular basis. Remote pairing works really fine if the network has no hiccup. We have the following simple setting:</p>
<ul>
<li>We setup a remote VM containing the development environment (compilers, source code) and accessible only to the team. Configuration of this VM is kept versioned in our code repository hence it follows the evolution of the code itself which may require over time different dependencies, different compilers… To be able to quickly setup a VM, we use a snapshot instead of full-blown configuration</li>
<li>People connect on the VM using with <a href="http://www.openssh.com/">ssh</a> and <a href="https://tmux.github.io/">tmux</a><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, and we use console-based <a href="https://www.gnu.org/software/emacs/">emacs</a> for hacking. OF course this is possible because we are using languages and tools that do not need a complex visual IDE to work<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>,</li>
<li>We use hangout for voice + video. Small tip, thanks to <a href="">Emmanuel Gaillot</a>: Set your terminal to be slightly transparent so that you can see the remote persons behind the code!</li>
</ul>
<p>I really like pair programming but I found its remote version is more exhausting than its face to face counterpart. It requires more focus and engagement because it is very easy to get distracted as your partner is not there to snoop over your shoulder and see you are actually browsing your emails or twitter. And the lower fidelity communication requires even more focus. However we found that timezone differences forced us to make something like <em>Deferred Pairing</em>: I start some work in France, using <a href="http://slack.com">slack</a> as a stream of consciousness, pushing code to github repository, then I go to sleep. In the middle of the night someone in Asia picks up where I left code and continue working on it till we manage to have some time overlap. Then we can pair for a couple of hours if needed, and restart the whole cycle.</p>
<h2 id="development-automation">Development Automation</h2>
<p><strong>Continuous Integration</strong> is core practice of XP and we embraced it fully from the onset of the project in the form of <em>continous deployment</em>: Each code change’s integration in the existing codebase is checked as soon as the code is published to be available for the whole team to use. Continuous should of course be understood in the mathematical sense, e.g. opposite of <strong>discrete</strong>: You are able to deploy at any point in time, not at certain gateways or releases milestones. I plan to provide more details in another blog post but practically speaking this means:</p>
<ul>
<li>People push to a “central” git repository located on a CI server. Note this repository is central only by virtue of being on the CI server and we could deploy other servers and “central” repositories at will. There is nothing special and we try to keep using git in a distributed way,</li>
<li>This triggers a job on the CI server which, among other things, runs automated tests (more on this later) and deploy an instance of the application for testing, and pushes the same instance to a central repository for deployment to production.</li>
</ul>
<p>This implies managing infrastructure, e.g. VMs, containers, processes and would not be possible without strong automation: <a href="http://www.jedi.be/blog/2013/05/24/Infrastructure%20as%20Code/">Infrastructure as Code</a> and DevOps everywhere, the slogan is: Automate all the things!</p>
<ul>
<li>We can rebuild all our infrastructure from a couple of git repositories
<ul>
<li><strong>Note</strong>: CI server is itself versioned and deployable</li>
<li>VM deployments is not fully automated yet but is scripted</li>
<li>Configuraiton management is versionned and handled in Haskell too</li>
</ul></li>
</ul>
<p>Continuous Integration is really our most important tool for maintaining cohesion in the team and integrity of the software: Any commit pushed to CI triggers whole chain of build down to the point we obtain a <em>deployable application container</em></p>
<h2 id="test-driven-development">Test Driven Development</h2>
<p>I have <a href="/posts/tdd.html">already written</a> on TDD and how much I loved it: To me TDD is <em>THE</em> core practice of XP and its main contribution to the advance of software development. Even in a startup setting I don’t think we made a lot of compromise on writing tests, but the amount of tests that need to be written in a strongly-typed language like Haskell is less than what you would need to write in Ruby or even Java.</p>
<p>Our automated tests are currently divided into 4 types:</p>
<ul>
<li><strong>Property based tests</strong>, expressing pure code’s properties like symmetry of read/write, description of processes, computations… There aren’t that many properties in our code, mostly I think because our domain is rather simple and not easily amenable to compact formulation of properties: There is no point in expressing a property in a way that is as complicated as the code it is supposed to specify,</li>
<li><strong>Integration tests</strong> check our code within the context of I/O, mostly through its REST interface but sometimes only using basic I/O or imperative code. This is where we catch most of our errors because it is very easy to make a mistake when interacting with the outside world, whereas the compiler and explicit types help a lot in catching internal programming mistakes,</li>
<li><strong>User Interface tests</strong> are actually unit tests for the UI code which is written in Clojurescript. These are the least developed part of our test stack which is a pity given the dynamic nature of the language we use and the amount of UI code we have, but we try to remove logic from UI as much as possible and most of the issues we have at this level are actually user interactions problems which can more quickly be caught through actually using the interface,</li>
<li><strong>End-to-End tests</strong> like their name implies tests the application at the highest-level using <a href="http://www.seleniumhq.org/">Selenium</a> to drive a browser to interact with the application like a human being would do. We have about 20 tests in this category and we try to keep that number low as obviously they are the more costly ones to run. However they are really invaluable in catching complex integration issues.</li>
</ul>
<h1 id="team">Team</h1>
<h2 id="on-being-remote">On Being Remote</h2>
<p>Working remotely has been an integral part of the organization since the beginning. We are 6-7 hours apart, e.g. it is afternoon in Singapore when I wake up. This means we have about 3-4 hours of overlap, but we span 16-17 hours of wall clock time. Here are some features that are entailed by being remote:</p>
<ul>
<li>This removes the need for a dedicated support/ops team, at least as long as we are smallish (see above notes about automation)</li>
<li>We do our stand-up meeting at 9am or 10am Paris Time, acts as a sync point,</li>
<li>We use overlap time for discussing stuff, pairing, solving design or hard technical problems. Hangout works ok but it would probably be better with dedicated software e.g. gotomeeting or equivalent)</li>
<li>We found that timezone differences are actually great for catching date/time related bugs!</li>
<li>We use <a href="http://slack.com">Slack</a> <strong>a lot</strong>. We could use IRC of course but it would require some more infrastructure and it is not as friendly to non tech-savvy persons:
<ul>
<li>We use ut to chat within the team and discuss technical stuff, post screnshots, code examples, design ideas…</li>
<li>We use it as a <strong>stream of consciousness</strong> endpoint: I dump what I am doing, jot down ideas, humours, feelings… When people wake up in the morning they have a trace of what I have done apart from the actual code/builds I produced and what I have been thinking of</li>
<li>We also use it as an endpoint for some important monitoring information, e.g. when something is pushed to central git repository or when state of a server changes.</li>
</ul></li>
</ul>
<p>There is a great post about <em>Remote First</em> culture: http://zachholman.com/posts/remote-first/ which you should read, along with 37 Signal’s <a href="http://37signals.com/remote">Remote</a>. They both emphasizes the fact that working remotely works <strong>if and only if</strong> the whole organisation is built around this premise. Think as a counter-example organisations that have a handful of offices which need to work distributedly. Distribution was an afterthought, something that came up as a constraint because of mergers, and no amount of tooling can make that kind of culture work.</p>
<p>Remote work can help reduce stress, especially in startups: Being remote means you can simply <em>remove</em> pressure from business people when it becomes too heavy. It can also help minimizes personality clash, cool down interactions. Being face-to-face increases cues and hints about the other person’s feelings but is also a great way to generate stress and emotions. The counterparty is <em>trust</em>: People at the other side of the world trust you to do what needs to be done</p>
<h2 id="hiring">Hiring</h2>
<p>The team is currently very small but hiring has been and will stay a team matter. Because we use a niche technology our recruitment is necessarily international and we must be prepared to recruit and work remotely, and actually <em>I was recruited as co-founder remotely…</em></p>
<p>Our hiring process is currently quite simple:</p>
<ul>
<li>Initial interview with me and/or another dev where I try to assess current skill level of candidate. Usually ask questions related to basic XP practices like TDD</li>
<li>2 hours pairing (or tripling) session using our remote dev environment: We usually tackle the <a href="https://github.com/emilybache/GildedRose-Refactoring-Kata/">GildedRoseKata</a> in Haskell, or if candidate is not comfortable enough in his favourite language (Javascript, Java…)</li>
<li>Interview with CEO and negotiations…</li>
<li>Here we go!</li>
</ul>
<p>Applying agile principles to hiring means that we are ready to experiment for a week or a month, reflect and improve, and possibly stop early (this occured already once). Doing this remotely also means more flexibility and more risks:</p>
<ul>
<li>If the person you hire wants to cheat, e.g. steal your code, she can do so quite easily,</li>
<li>Developers are freelance in their country, which means they can stop at any time,</li>
<li>Hence finally there is an incentive for Capital Match to keep people happy and motivated.</li>
</ul>
<p>Hiring is one of those (numerous) areas where I see a lot of improvements and where being remote is a game changer.</p>
<h1 id="theorizing">Theorizing</h1>
<p>Lessons learnt are most interesting if you can leverage them into some form of theoretical knowledge that can be turned into useful guidelines for future decisions. This section tries to extract some possibly useful insights and generalisations from this limited experience, which I can summarize in three points:</p>
<ul>
<li>Treating software as <em>development projects</em> alone is a fallacy that leads to costly errors,</li>
<li><em>Technology matters</em>,</li>
<li>Our software is part of a <em>system</em> that has different requirements at different moments in time.</li>
</ul>
<h2 id="the-software-development-project-fallacy">The Software Development Project Fallacy</h2>
<p>There is not a dearth of software development project management methods, whether Agile or not: Kanban, Scrum, RUP, Waterfall, V, W, Design Controls… And symetrically there are quite a few methods targeted at managing operations and support in IT, like ITIL, although I am much less familiar with those. The fact is they are all inspired by the logic of <em>manufactured goods design and production</em> where:</p>
<ol style="list-style-type: decimal">
<li>The good undertakes a series of transformations, from design to production, up to the point it is ready for consumption by the “public”,</li>
<li>Where it falls into the hands of another group of people which is responsible for <em>maintenance</em> or <em>end-users support</em>.</li>
</ol>
<p>If we focus on Scrum alone, we find the core artifact to be the <strong>sprint</strong> and a number of sprints ultimately leading to a <strong>release</strong>. But sprints, and for that matter any form of time-constrained activity, are inherently exhausting and ultimately unsustainable, hence the mutiplications of <em>hardening sprints</em>, <em>release sprints</em>, <em>sprint 0</em>… which are all attempts to depart from the sprint/release straightjacket. What Scrum and even XP which is dearer to my heart misses is the <em>Operations</em> part of a system</p>
<blockquote>
<p>Scrum is a management and control process that cuts through complexity to focus on <em>building software</em> that meets business needs. Management and teams are able to get their hands around the requirements and technologies, never let go, and deliver working software, incrementally and empirically.</p>
<p>Scrum itself is a simple framework for effective team collaboration on complex <em>software projects</em>.</p>
<p><a href="https://www.scrum.org/resources/what-is-scrum">Scrum.org</a></p>
</blockquote>
<p>The way of thinking this methods promote is inherently constrained by a time-bound horizon: The moment the product will be considered “finished” and leave the organisation that produced it, hence the emphasis of the <strong>release</strong>. Lets remember that <em>Deliver</em><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> also means <em>give birth</em> and bears with it the idea that something we have been fostering for months is freed from our control, which also suggests we are freeing ourselves of it. Deliver’s substantive is <a href="http://www.wordreference.com/enfr/deliverance">deliverance</a>… The <em>release</em> becomes the horizon of the team and with it comes relief, and then we move on to something else.</p>
<p>What I have learnt over the years but even more so over the past months working on this project is the importance of evaluating your capability to deliver features against your capability to maintain your software and make it able to sustain more change in the future. Viewing software systems solely under the angle of <em>projects</em> leads to a bias, from all persons involved, towards delivering more under time pressure which is the biggest source of errors. When your horizon is time constrained you cannot pay enough attention to the small warnings that will lead to big failures.</p>
<h2 id="technology-matters">Technology matters</h2>
<blockquote>
<p>“The technology, stupid.”</p>
<p>liberally adapted from <a href="https://en.wikipedia.org/wiki/It&#39;s_the_economy,_stupid">James Carville</a></p>
</blockquote>
<p>Obviously a lot of the things we do are made possible thanks to technology:</p>
<ul>
<li>Being distributed is now workable because there are quite a few affordable tools out there that make this style of working possible for small teams: Google Hangouts, Skype, Slack/IRC, mails of course, Git, SSH, Cloud providers, Docker, Linux are all key ingredients to make our distributed workplace possible<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>,</li>
<li>Developing our business as an online platform is also a key factor in enabling both an <em>iterative</em> and inherently <em>agile</em> style of software but also in the availability of tools and systems. It is much harder to develop an embedded controller’s system in a remote way because your software is physically tied to something concrete,</li>
<li>The free and/or open-source movement has popularised geographically distributed teams and flat hierarchies for developing even very large pieces of software.</li>
</ul>
<p>But I do think that our particular technology stack (e.g. Haskell) choice has a deep impact in enabling this distributed agile style of working:</p>
<ul>
<li>Haskell is (still) a <a href="http://githut.info/">niche language</a> for at least two reasons: Because of its paradigm which is very different from maintstream languages and platforms, and because of its history which ties it to academic and esoteric PLT research more than to the production of mundane web applications, <img src="/images/haskell-ranking.png" alt="Latest Ranking from http://githut.info" /></li>
<li>Due to its niche nature, it attracts programmers that either want to distinguish themselves and/or are inherently attracted by the different, not to say the bizarre… This is often the kind of people that are ready to make some extra efforts to keep on working with such a language, including working with shifted hours, moving to a foreign country, learning new tools and processes…</li>
<li>On a more technical side, this particular choice allows us to work mostly through lightweight text-based tools, e.g. terminals, text editors, SSH… something which is invaluable when you have to communicate over 10000 kilometers and flacky Internet.</li>
</ul>
<p>Technology is not neutral. As explained by <a href="/posts/eme.html">Bruno Latour</a>, technology is embodied into technical beings whose needs must be adressed in specific ways and with whom we need to interact in specific ways. Technology shapes our worldview and the way we work in a lot of different and sometimes surprising ways. But it is always worthwhile to think about it beforehand…</p>
<h2 id="from-project-to-system">From Project to System</h2>
<p>My experience suggests a different approach which does not reject the benefits of iterative software <em>development</em> but makes it part of a bigger process which also encompasses the need for periods of reflection and calm. We should acknowledge the duality (at least) in our processes:</p>
<ul>
<li><strong>Period I</strong>: Moments of <em>intense</em> activity, which might be due to deadlines, objectives, production incidents to solve, security issues…</li>
<li><strong>Period II</strong>: Moments of <em>routine</em> activity where team can reflect, define processes and procedures, refine things, document…</li>
</ul>
<p>Agile methods are good at channelling the intensity of period I through a simple process but they are much less appropriate for period II which is mostly characterized by the absence of definite goal(s). This explains the rise of “hybrid” approaches and the success of Kanban which offers a much more appropriate framework to handle period II work (and conversely is not very well suited to period I…). This also explains the moderate success (at best) Agile methods have with operations or highly regulated settings.</p>
<p>There is a lot to get inspired from the way <a href="http://high-reliability.org/">Highly Reliable Organisations</a> work and the numerous research studies that have been written about those organisations. I was also personally heavily influenced by <a href="/posts/decisions-absurdes.html">“Les décisions absurdes”</a> which describes the mechanics of group thinking and how they can lead to catastrophic failures. HRO studies show that those organisations which <strong>must</strong> be reliable actually acknowledge the existence of those two modes of operations. The best such organisations know how to take advantage of <em>routine</em> mode (what I call Period II) to become better when the need arises to go into <em>intense</em> mode or Period I.</p>
<p>Think of those analogies:</p>
<ul>
<li>In sport, athletes do not spend 100% of their time competing, and actually competition accounts for a small fraction of their worktime, most of it is dedicated to training, learning, improving, reflecting on past competitions,</li>
<li>Musicians and other performers do the same: A lot of time training and practicing, small fraction of their time on stage,</li>
<li>Soldiers and military are an even better analogy because period of conflicts can last for an extended period of time and even then, army takes care of rotating personel, replacing people on the front with fresh troops on a regular basis.</li>
</ul>
<blockquote>
<p>The content of this post was used to present a session at <a href="http://www.agilenantes.org/agile-tour-nantes-2015/">Agile Tour Nantes 2015</a>: Thanks to the organisers for having accepted that talk.</p>
<p>Thanks to Anne Caron-Perlon for her very helpful comments and remarks.</p>
<p>Thanks to Capital Match Team for those exciting 2 years.</p>
</blockquote>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>It seems that <a href="http://tmate.io/">tmate</a> provides even better support for that at the expense of having to setup a centralised server for people to log in.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Looks like the smart people at /ut7 have given a name to this: They call it <a href="http://ut7.fr/posts/blog/2015/03/05/les-pieds-dans-la-bassine.html">La bassine</a> and apparently this has been developed as part of work on project for <a href="http://blog.deliverous.com/2015-03-06.bassine.html">Deliverous</a><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This comes from the French <em>Délivrer</em> which can be translated to <em>get rid of</em>, <em>relieve</em>, <em>set free</em><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Back in the 90s or even early 2000s you would have to be either a large corporation, a university or deep hacker group to be able to work distributedly in real time<a href="#fnref4">↩</a></p></li>
</ol>
</div>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>Sur &quot;Spinoza encule Hegel&quot;</title>
    <link href="http://abailly.github.io/posts/spinoza-encule-hegel.html" />
    <id>http://abailly.github.io/posts/spinoza-encule-hegel.html</id>
    <published>2016-04-23T00:00:00Z</published>
    <updated>2016-04-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>Sur &quot;Spinoza encule Hegel&quot;</h1>
<div class="info">Posted on April 23, 2016</div>

<p>Une France post-apocalyptique et comme suspendue. Des gangs sillonent l’ex-capitale et le pays pour s’entretuer. Ils ont nom <em>Sang Noir de Bakounine</em>, <em>Docteur Jdanov</em>, <em>Planète Potlatch</em>, <em>Jeunes Hégeliens</em>… Julius Puech est le chef de la <em>Fraction Armée Spinoziste</em>. Il porte des bottes de lézard mauves et voue une haine farouche aux Hégeliens dont le chef s’appelle Thorez. Il aime Spinoza et François. Il a de la sympathie pour le Diable. Tout ça finira très mal.</p>
<div class="figure">
<img src="/images/spinoza-encule-hegel.jpg" class="floating" height="300" />

</div>
<p>Plutôt que <em>La septième fonction du langage</em> de Laurent Binet qui y fait référence, mieux vaut lire <a href="https://fr.wikipedia.org/wiki/Spinoza_encule_Hegel">Spinoza encule Hegel</a> de Jean-Bernard Pouy. Si possible vite, très vite. Avec une bouteille d’alcool fort à portée de la main, et les Stones à fond dans les oreilles. Ou alors entre deux propositions de l’Éthique. Mais le texte n’a pas besoin d’adjuvants pour exciter les sens, enivrer. La prose est hypnotique et balistique, mais sait se faire lyrique quand elle évoque les amours - exclusivement masculines - des héros. Manière élégante somme toute de dépasser le sexisme inhérent aux situations riches en testostérone. D’ailleurs, le héros finira esclave d’un gang exclusivement féminin pas moins nihiliste que ses congénères masculins.</p>
<p>Métaphore sous acide d’un mai 68 qui s’éloigne et dont on sait les fruits amers, SEH est un livre coup-de-poing-dans-le-bide autant qu’une blague de potache. Un éclat de rire autant qu’un chant mélancolique. Je ne sais pas si je l’aime par nostalgie d’une époque fantasmée, par regret d’être venu trop tard, à l’époque de la Grande Récupération - ou de la Grande Trahison - après que les plus malins eurent transformés le capital intellectuel accumulé dans les seventies en capital tout court.</p>
<p>Ou parce qu’il me reste encore un peu de la rage adolescente.</p>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>First Steps with MirageOS</title>
    <link href="http://abailly.github.io/posts/mirage-os-newbie.html" />
    <id>http://abailly.github.io/posts/mirage-os-newbie.html</id>
    <published>2016-04-22T00:00:00Z</published>
    <updated>2016-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>First Steps with MirageOS</h1>
<div class="info">Posted on April 22, 2016</div>

<p>This post is a follow-up to the session I gave at <a href="http://cfp.devoxx.fr/2016/talk/KEN-7124/Unikernel:_Le_conteneur_du_futur">Devoxx France</a> on <a href="http://mirage.io">MirageOS</a>. It provides detailed instructions on how to setup a development environment to be able to:</p>
<ul>
<li>Build locally Mirage projects and generate native binaries with <code>--unix</code> configuration flag,</li>
<li>Install and configure a VirtualBox VM that runs Xen Hypervisor,</li>
<li>Build Xen-enabled Mirage unikernels on this VM.</li>
</ul>
<h2 id="references">References</h2>
<p>Those points are already covered in other documentation material available here and there on the web and I mainly did some synthesis from a <em>newbie</em> perspective. Here is a list of resources that were helpful in making this post:</p>
<ul>
<li><a href="https://mirage.io/wiki/">Official MirageOS Wiki</a> which contains various tutorials,</li>
<li><a href="https://github.com/mirage/mirage-www/">mirage-www</a>, the self-hosted website,</li>
<li><a href="http://www.skjegstad.com/blog/2015/01/19/mirageos-xen-virtualbox/">Magnus Skjegstad</a> blog post which mostly covers point 2. above, and <a href="http://www.skjegstad.com/blog/2015/03/25/mirageos-vm-per-url-experiment/">another post</a> which covers more advanced stuff,</li>
<li><a href="https://www.somerandomidiot.com/blog/2014/07/25/doing-nothing-in-mirage/">Doing nothing in Mirage</a>, an older post on basic configuration,</li>
<li>An account on recent Solo5 development to <a href="https://mirage.io/blog/introducing-solo5">run Mirage on Qemu</a>,</li>
<li>Another <em>getting started</em> <a href="http://roscidus.com/blog/blog/2014/07/28/my-first-unikernel%2F">blog post</a></li>
<li>Some random pages on Xen configuration:
<ul>
<li><a href="http://superuser.com/questions/737976/how-do-i-get-xen-domu-guests-to-have-internet-access">Accessing the Internet from domU kernels</a>,</li>
<li><a href="http://wiki.xenproject.org/wiki/Host_Configuration/Networking">Networking in Xen</a>, not for the faint of heart,</li>
<li>Related work for <a href="https://github.com/GaloisInc/HaLVM/wiki/Building-a-Development-Virtual-Machine">HaLVM</a>, the unikernel system in Haskell.</li>
</ul></li>
</ul>
<h1 id="a-naive-authentication-service">A Naive Authentication Service</h1>
<h2 id="goal">Goal</h2>
<p>The goal of this post is to build a simple HTTP service that can authenticate users based on their password. This service’s specification is simple, not to say simplistic and naive:</p>
<ul>
<li>It maintains a key/value store from user names to passwords (we do not care about encryption here but of course they should be),</li>
<li>It receives <code>GET</code> HTTP requests on port 80 and expects two parameters in the query part: A <code>user</code> string and a <code>password</code> string,</li>
<li>If the given password for given user matches stored password, it responds with <code>200 OK</code>,</li>
<li>In all other cases it should return <code>401 UNAUTHORIZED</code>.</li>
</ul>
<p>To build this service we need to:</p>
<ul>
<li>Run a HTTP Server: I will use <a href="https://github.com/mirage/ocaml-cohttp">ocaml-cohttp</a> which is built specifically to be run within MirageOS applications,</li>
<li>Store data in some key/value store: MirageOS provides <code>kv_ro</code> which is an abstract key/value store which can be backed in code or in a FAT-formatted disk image.</li>
</ul>
<h2 id="server-code">Server Code</h2>
<p>The server code resides in a <code>unikernel.ml</code> file. I am not at all familiar with OCaml so my code is probably not really idiomatic. Learning OCaml is one of the reasons I became interested in MirageOS in the first place !</p>
<p>We first need some usual incantations to “import” names, e.g. opening modules in OCaml:</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span class="ot">open</span> String
<span class="ot">open</span> Lwt
<span class="ot">open</span> Lwt<span class="kw">.</span>Infix
<span class="ot">open</span> V1_LWT
<span class="ot">open</span> Printf</code></pre></div>
<p>The <code>string_of_stream</code> function is adapted from the basic <code>kv_ro</code> example provided in <a href="https://github.com/mirage/mirage-skeleton">mirage-skeleton</a>. Its name is pretty explicit…</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span class="kw">let</span> string_of_stream stream =
  <span class="kw">let</span> s = List<span class="kw">.</span>map Cstruct<span class="kw">.</span>to_string stream <span class="kw">in</span>
  return (String<span class="kw">.</span>concat <span class="st">&quot;&quot;</span> s)</code></pre></div>
<p>The main module is parameterized by 3 signatures which are provided by MirageOS according to the configuration given at build-time. Here we have 3 modules: The console for outputting some logs, the Key/Value store and <code>conduit</code> which provides network connectivity:</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span class="ot">module</span> Main (<span class="dt">C</span>:<span class="dt">CONSOLE</span>) (<span class="dt">K</span>: <span class="dt">KV_RO</span>) (<span class="dt">CON</span>:Conduit_mirage<span class="kw">.</span><span class="dt">S</span>) = <span class="kw">struct</span></code></pre></div>
<p>We build a new module by passing underlying connection manager to our HTTP server implementation then we can define our <code>start</code> entry point:</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml">  <span class="ot">module</span> H = Cohttp_mirage<span class="kw">.</span><span class="dt">Server</span>(Conduit_mirage<span class="kw">.</span><span class="dt">Flow</span>)

  <span class="kw">let</span> start console k conduit =</code></pre></div>
<p>We first define the function that will check the database for matching username/password pair, given an access request. The main thing to note here is that reading from the store happens within the context of so-called <em>light-weight threads</em> or <code>Lwt</code> which is the basic Mirage cooperative thread execution model. Potentially blocking operations expect a <em>continuation</em> that will be passed the result of the operation.</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml">    <span class="kw">let</span> check_pwd u p =
      K<span class="kw">.</span>read k u <span class="dv">0</span> <span class="dv">4096</span> &gt;&gt;= <span class="kw">fun</span> readpwd -&gt;
      <span class="kw">match</span> readpwd <span class="kw">with</span>
      | <span class="dt">`Ok</span> pwds -&gt;
         string_of_stream pwds &gt;&gt;= <span class="kw">fun</span> pwd -&gt;
         return $ compare (trim pwd) p == <span class="dv">0</span>
      | _ -&gt; return <span class="kw">false</span>
    <span class="kw">in</span> </code></pre></div>
<p>Then comes the meat of the server, the <code>http_callback</code> function that will be called by cohttp upon everyrequest. We extract interesting parts of the request’s URI then check whether or not the username/password pair matches, and finally return a response.</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml">    <span class="kw">let</span> http_callback _conn_id req _body =
      <span class="kw">let</span> uri  = Cohttp<span class="kw">.</span>Request<span class="kw">.</span>uri req             <span class="kw">in</span>
      <span class="kw">let</span> path = Uri<span class="kw">.</span>path uri                       <span class="kw">in</span>
      <span class="kw">let</span> user = Uri<span class="kw">.</span>get_query_param uri <span class="st">&quot;user&quot;</span>     <span class="kw">in</span>
      <span class="kw">let</span> pwd  = Uri<span class="kw">.</span>get_query_param uri <span class="st">&quot;password&quot;</span> <span class="kw">in</span>
      <span class="kw">match</span> (user,pwd) <span class="kw">with</span>
      | (<span class="dt">Some</span> u, <span class="dt">Some</span> p) -&gt;
          check_pwd u p &gt;&gt;= <span class="kw">fun</span> chk -&gt; 
          <span class="kw">if</span> chk
          <span class="kw">then</span> H<span class="kw">.</span>respond_string ~status:<span class="dt">`OK</span> ~body:(sprintf <span class="st">&quot;hello %s!</span><span class="ch">\n</span><span class="st">&quot;</span> u) ()
          <span class="kw">else</span> H<span class="kw">.</span>respond_string ~status:<span class="dt">`Unauthorized</span> ~body:<span class="st">&quot;Invalid login</span><span class="ch">\n</span><span class="st">&quot;</span> ()
      | _                -&gt;
          H<span class="kw">.</span>respond_string ~status:<span class="dt">`Unauthorized</span> ~body:<span class="st">&quot;No user given</span><span class="ch">\n</span><span class="st">&quot;</span> ()
    <span class="kw">in</span></code></pre></div>
<p>It remains to start the ball rolling by creating the HTTP server and listening on some port, using underlying <code>CON</code>duit module implementation:</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml">    <span class="kw">let</span> spec = H<span class="kw">.</span>make ~callback:http_callback () <span class="kw">in</span>
    CON<span class="kw">.</span>listen conduit (<span class="dt">`TCP</span> <span class="dv">8088</span>) (H<span class="kw">.</span>listen spec)
end</code></pre></div>
<p>The single most important thing to note here is that this code is pretty much system-agnostic: There is no dependency on a particular low-level runtime infrastructure and the same code can be run either as a native binary on Unix or packaged as unikernel on Xen (or QEMU). Implementation details are hidden behind the signatures, e.g. interfaces, that parameterize the <code>Main</code> module and whose instances are passed to the <code>start</code> function.</p>
<h2 id="configuring-mirage">Configuring Mirage</h2>
<p>The second important file one has to write for a MirageOS application is the <code>config.ml</code> file which is used by <code>mirage</code> tool to generate the needed boilerplate to run the actual code.</p>
<p>After the incantation to access <code>Mirage</code> module’s content, we define our first component that will be melded in our application. This one is a simple generic key/value store whose content will be built from the <code>data/</code> directory. In Unix mode requests for some key will be matched to the content of the <code>data/</code> directory, whereas in Xen mode we will need to pack that data inside a block device and mount that device in the built kernel</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span class="ot">open</span> Mirage

<span class="kw">let</span> auth_data = generic_kv_ro <span class="st">&quot;data&quot;</span></code></pre></div>
<p>Our second component is the HTTP server. We can see here our configuration depends on the underlying platform. Note that the <code>generic_stackv4</code> provides IPv4 implementation for both Unix and Xen and strictly speaking we could get rid of the <code>if_impl</code> construct. It is also possible to use a <code>direct_stackv4_with_default_ipv4</code> implementation on Xen: This will assign a static IP to the kernel which can be changed when configuring mirage.</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span class="kw">let</span> server =
  <span class="kw">let</span> network =
    if_impl Key<span class="kw">.</span>is_xen
            (generic_stackv4 default_console tap0)
            (socket_stackv4 default_console [Ipaddr<span class="kw">.</span>V4<span class="kw">.</span>any]) <span class="kw">in</span>
  conduit_direct network</code></pre></div>
<p>The <code>handler</code> function defines the general structure of our service and adds some libraries and packages to be imported by mirage. Then we can finally register our <code>auth_service</code> passing it the result of invoking handler with actual implementation of the modules it requires.</p>
<div class="sourceCode"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span class="kw">let</span> handler =
  <span class="kw">let</span> libraries = [<span class="st">&quot;mirage-http&quot;</span>] <span class="kw">in</span>
  <span class="kw">let</span> packages = [<span class="st">&quot;mirage-http&quot;</span>] <span class="kw">in</span>
  foreign
    ~libraries ~packages
    <span class="st">&quot;Unikernel.Main&quot;</span> (console @-&gt; kv_ro @-&gt; conduit @-&gt; job)

<span class="kw">let</span> () =
  register <span class="st">&quot;auth_service&quot;</span> [handler $ default_console $ auth_data $ server]</code></pre></div>
<p>I must confessh this part is still a bit mysterious to me…</p>
<h2 id="building-on-unix">Building on Unix</h2>
<p>We will assume <a href="https://mirage.io/wiki/install">mirage is installed</a> according to the instructions. At the time of this writing, we use:</p>
<pre><code>$ opam --version
1.2.2
$ ocamlc -version
4.02.3
$ mirage --version
2.8.0</code></pre>
<p>We can first try to build and run our service on Unix, e.g. locally. We first need to populate a <code>data/</code> directory with some files representing usernames and add a password inside each file. Then we can configure Mirage, build the executable using the generated <code>Makefile</code> and run it.</p>
<pre><code>$ mirage configure -vv --unix
...
$ make
$ ./mir-auth_service
Manager: connect
Manager: configuring</code></pre>
<p>In another console, we can test that our authentication service works correctly:</p>
<pre><code>$ curl -v http://localhost:8088/foo/bar?user=john\&amp;password=password
&lt; HTTP/1.1 200 OK
&lt; content-length: 12
&lt; 
hello john!
$ curl -v http://localhost:8088/foo/bar?user=john\&amp;password=passwor
&lt; HTTP/1.1 401 Unauthorized
&lt; content-length: 14
&lt; 
Invalid login</code></pre>
<p>So far, so good! We are now ready to test our unikernel for “real”, e.g. to configure a VM running Xen in VirtualBox…</p>
<h1 id="configuring-a-xen-vm-on-virtualbox">Configuring a Xen VM on Virtualbox</h1>
<blockquote>
<p><strong>Note</strong>: This has only been tested on the Ubuntu 14.04 box</p>
</blockquote>
<p>Fortunately for us, all the grunt work has already been done in the <a href="https://github.com/abailly/mirage-vagrant-vms">mirage-vagrant-vms</a> which I simply forked and extended to add the following features:</p>
<ul>
<li>Install the latest <em>Official</em> version of ocaml toolchain and mirage environment,</li>
<li>Configure a <em>host-only</em> network for the VM,</li>
<li>Configure <em>bridge</em>, <em>dnsmasq</em> and <em>IP forwarding</em> in the VM to be used by Xen domU kernels.</li>
</ul>
<h2 id="networking">Networking</h2>
<p>The part I had most difficulties with is of course the networking part. We want the <em>domU</em> virtual machines running on top of Xen hypervisor inside the VirtualBox VM to be visible from our <em>host</em>. The following picture tries to render my understanding of the network topology and configuration we want to achieve (actual IPs/MACs may of course differ).</p>
<div class="figure">
<img src="/images/xen-vm-network-topo.png" />

</div>
<p>The VirtualBox engine exposes the host-only interface to the host as <code>vboxnet4</code> with a pre-configured IP (e.g. IP is set in <code>Vagrantfile</code> as part of definition of host-only network). This interface is linked to the <code>eth1</code> “physical interface” inside the VM and part of the same <em>ethernet</em> network. We create a <code>br0</code> interface which is a bridge: An interface that connects two or more interfaces by routing the packets to/from each of the bridged interfaces. In this case it connects the host-only interface (<code>eth1</code>) and the the virtual interfaces created for each domU kernel, in this case <code>vif1.1</code>. The latter is pair of a pair of virtual interfaces created by underlying hypervisor when the domU kernel boots up, the other member of the pair being the <code>tap0</code> interface on the domU side. Each packet going through either interface is made available to the other interface.</p>
<p>Another important is configuring and activating <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</a> to ensure domU kernels will be able to get an IP address using DHCP. Here we configure it to be attached to <code>br0</code> interface - our bridge - and to serve IPs from 192.168.77.3 to 192.168.77.200.</p>
<p>Last but not least, one must not forget to enable <strong>promiscuous mode</strong> on the host-only NIC created for the VM: I spent a couple hours trying to understand why my packets could not reach the unikernel although I could see ARP frames being exchanged…</p>
<h2 id="building-mirageos-kernel-on-xen-enabled-vm">Building MirageOS kernel on Xen-enabled VM</h2>
<p>We are now ready to configure, build and run our unikernel on Xen VM. We assume code has been somehow uploaded to the VM:</p>
<pre><code>$ mirage configure --xen -vv --net direct --dhcp true --tls false --network=0
...
$ make</code></pre>
<p>This creates a bunch of new files in the project’s directory, the most important one being <code>auth_service.xl</code>:</p>
<pre><code># Generated by auth_service (Wed, 20 Apr 2016 13:53:27 GMT).

name = &#39;auth_service&#39;
kernel = &#39;/Users/arnaud/projects/unikernel/auth-service/mir-auth_service.xen&#39;
builder = &#39;linux&#39;
memory = 256
on_crash = &#39;preserve&#39;

disk = [ &#39;format=raw, vdev=xvdb, access=rw, target=/Users/arnaud/projects/unikernel/auth-service/tar_block1.img&#39;, &#39;format=raw, vdev=xvdc, access=rw, target=/Users/arnaud/projects/unikernel/auth-service/fat_block1.img&#39; ]

# if your system uses openvswitch then either edit /etc/xen/xl.conf and set
#     vif.default.script=&quot;vif-openvswitch&quot;
# or add &quot;script=vif-openvswitch,&quot; before the &quot;bridge=&quot; below:
vif = [ ]</code></pre>
<p>This is the descriptor that is passed to the Xen launcher to start the unikernel. We must also not forget to build the disk image that will be loaded in our unikernel and exposed to our program as a key/value store:</p>
<pre><code>$ ./make-fat_block1-image.sh
$ sudo xl -v create -c auth_service.xl</code></pre>
<p>The <code>-v</code> option asks <code>xl</code> to be verbose and the <code>-c</code> attaches a console to the launched kernel.</p>
<p>This does not work out of the box and I needed to manually edit the generated files to fix a couple of minor issues:</p>
<ul>
<li>The <code>disk</code> section of <code>auth_service.xl</code> contains 2 disks but we only generate one of them, the <code>fat_block1.img</code>: The <code>tar_block1.img</code> must be removed from the list,</li>
<li>The file <code>auth_service.xe</code> also contains some descriptor for the same image, which needs to be also removed,</li>
<li>The <code>vif = []</code> part should also be replaced by <code>vif = [ &quot;bridge=br0&quot; ]</code> to ensure network device for unikernel is attached to the bridge and can actually be assigned an IP through dnsmasq,</li>
<li><code>memory = 256</code> is also way too much: Replace with <code>memory = 32</code> which should be enough (16 does not work…),</li>
<li>There is a slight error in the <code>make-fat_block1-image.sh</code>: The block size used is made equal to the total size of the files to include in the image, which does not seem to make much sense. I set it to be 32Kb.</li>
</ul>
<p>With this modifications one should expect to see something like the following being printed to the console:</p>
<pre><code>MirageOS booting...
Initialising timer interface
Initialising console ... done.
getenv(OCAMLRUNPARAM) -&gt; null
getenv(CAMLRUNPARAM) -&gt; null
getenv(PATH) -&gt; null
Unsupported function lseek called in Mini-OS kernel
Unsupported function lseek called in Mini-OS kernel
Unsupported function lseek called in Mini-OS kernel
getenv(OCAMLRUNPARAM) -&gt; null
getenv(CAMLRUNPARAM) -&gt; null
getenv(TMPDIR) -&gt; null
getenv(TEMP) -&gt; null
Netif: add resume hook
Netif.connect 0
Netfront.create: id=0 domid=0
 sg:true gso_tcpv4:true rx_copy:true rx_flip:false smart_poll:false
MAC: 00:16:3e:2e:c2:ce
Attempt to open(/dev/urandom)!
Unsupported function getpid called in Mini-OS kernel
Unsupported function getppid called in Mini-OS kernel
Manager: connect
Manager: configuring
DHCP: start discovery

Sending DHCP broadcast (length 552)
DHCP response:
input ciaddr 0.0.0.0 yiaddr 192.168.77.157
siaddr 192.168.77.2 giaddr 0.0.0.0
chaddr 00163e2ec2ce00000000000000000000 sname  file 
DHCP: offer received: 192.168.77.157
DHCP options: Offer : DNS servers(192.168.77.2), Routers(192.168.77.2), Broadcast(192.168.77.255), Subnet mask(255.255.255.0), Unknown(59[4]), Unknown(58[4]), Lease time(3600), Server identifer(192.168.77.2)
Sending DHCP broadcast (length 552)
DHCP response:
input ciaddr 0.0.0.0 yiaddr 192.168.77.157
siaddr 192.168.77.2 giaddr 0.0.0.0
chaddr 00163e2ec2ce00000000000000000000 sname  file 
DHCP: offer received
                    IPv4: 192.168.77.157
                                        Netmask: 255.255.255.0
                                                              Gateways: [192.168.77.2]
ARP: sending gratuitous from 192.168.77.157
DHCP offer received and bound to 192.168.77.157 nm 255.255.255.0 gw [192.168.77.2]
Manager: configuration done</code></pre>
<p>And if now query our service from the host, it should reply:</p>
<pre><code>$ curl -v http://192.168.77.157:8088/foo/bar?user=john\&amp;password=password
*   Trying 192.168.77.157...
* Connected to 192.168.77.157 (192.168.77.157) port 8088 (#0)
&gt; GET /foo/bar?user=john&amp;password=password HTTP/1.1
&gt; Host: 192.168.77.157:8088
&gt; User-Agent: curl/7.43.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; content-length: 12
&lt; 
hello john!
* Connection #0 to host 192.168.77.157 left intact</code></pre>
<p>With the corresponding trace on the server side being:</p>
<pre><code>ARP responding to: who-has 192.168.77.157?
ARP: transmitting probe -&gt; 192.168.77.1
ARP: updating 192.168.77.1 -&gt; 0a:00:27:00:00:04
Check password for &#39;john&#39; is &#39;password&#39;
Found password for &#39;john&#39;: &#39;password&#39;
ARP: timeout 192.168.77.1</code></pre>
<p>We have been running successfuly our first home-made unikernel on an hypervisor!</p>
<h1 id="conclusion">Conclusion</h1>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>MirageOS is quite well-packaged and thought out and mostly works out-of-the-box as advertised. The documentation is detailed and takes the newbie by the hand through the various steps one needs to build all the examples. All in all the “developer experience” is better than with <a href="https://github.com/GaloisInc/HaLVM">HaLVM</a>,</li>
<li>I was mostly frustrated by my lack of knowledge and understanding of Xen platform and virtual networking and I am indebted to people from the <a href="http://lists.xenproject.org/cgi-bin/mailman/listinfo/mirageos-devel">MirageOS mailing list</a> and <a href="http://freenode.net/">IRC Channel #mirage</a> for their help,</li>
<li>I was also limited by my lack of knowledge of OCaml, but working with MirageOS is a good incentive to learn the language which seems interesting and somehow <em>feels</em> different from Haskell while sharing a lot of concepts.. Finding documentation on OCaml is pretty straightforward and idioms are close enough to Haskell I was not too far off most of the time…</li>
</ul>
<h2 id="next-steps">Next steps</h2>
<p>I really think the unikernel paradigm could be as important a shift as containers and dockers have been in the last couple of years and as such I really want to keep investigating what MirageOS and HaLVM have to offer. To the best of my understanding there seems to be two different approaches to unikernels:</p>
<ul>
<li>A <em>bottom-up</em>, <em>language-agnostic</em> and <em>system-based</em> approach which produces unikernels for any or most kind of unix binaries by stripping down a stock OS and packing it along with adapted libs with the target process. This approach is merely an extension of the existing containers paradigm and is exemplified by <a href="http://rumpkernel.org/">Rump kernels</a> and <a href="http://osv.io/">OSv</a>,</li>
<li>A <em>top-down</em> and <em>language-centric</em> approach exemplified by Mirage and HaLVM where system-level components are actually written in the language thus providing all the benefits of tight integration and focus, and packed with a custom minimalistic OS.</li>
</ul>
<p>Obviously, the former approach definitely requires less effort and allows one to produce unikernels in a very short time span by simply repackaging existing applications and services. The latter approach is more demanding as it requires expertise in a specific language and building all the needed components, sometimes from scratch, to provide low-level services otherwise provided by standard system libraries. I don’t have much rational arguments, beyond those provided in Mirage’s own documentation, to back my personal preference for the latter: I might be biased by my interest in functional programming languages, especially strongly typed ones, and the somewhat childish desire to master every single part of the system stack.</p>
<p>What I want to do know is:</p>
<ul>
<li>Write a truly useful service that could be unikernelised in OCaml: To build on some previous experiments in the domain of distributed consensus, I think having a unikernel-based Raft implementation could be a challenging and interesting next step. Another potentially interesting use case would be to build unikernels for serverless tasks dispatched with something like <a href="http://aws.amazon.com/fr/documentation/lambda/">AWS Lambda</a> but this is not possible,</li>
<li>Experiment more with <a href="https://github.com/djwillia/solo5">solo5</a>, a companion project of Mirage whose goal is to allow running Mirage unikernels on top of non-Xen hypervisors, like qemu and VirtualBox,</li>
<li>Experiment with deployment of unikernels on cloud providers. There is some documentation and script already available to <a href="https://mirage.io/wiki/xen-boot">deploy to AWS</a> which I would like to try.</li>
</ul>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>On &quot;Cakes, Custard + Category Theory&quot;</title>
    <link href="http://abailly.github.io/posts/cake-custard-category.html" />
    <id>http://abailly.github.io/posts/cake-custard-category.html</id>
    <published>2016-04-18T00:00:00Z</published>
    <updated>2016-04-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>On &quot;Cakes, Custard + Category Theory&quot;</h1>
<div class="info">Posted on April 18, 2016</div>

<p>I am not a mathematician, and this is probably the only regret of my life. When I was young I was too lazy to work seriously enough in High School to be accepted at one of the “Grandes Écoles”. Or rather I was way too much interested by role-playing games to spend time doing homework. And I must say maths were quite boring in High School and Preparatory School, not even talking about Business school’s math which barely went beyond basic arithmetics…</p>
<p>Things would probably have been quite different had I been taught math by <a href="http://www.eugeniacheng.com/">Eugenia Cheng</a>, the author of <a href="http://www.amazon.co.uk/Cakes-Custard-Category-Theory-understanding/dp/1781252874/ref=sr_1_1?ie=UTF8&amp;qid=1422244697&amp;sr=8-1&amp;keywords=eugenia+cheng">Cakes, Custard + Category Theory</a> and one of the presenter of the famous <a href="https://www.youtube.com/watch?v=9fohXBj2UEI&amp;list=PL0E91279846EC843E">Catsters</a> series, among <a href="http://liederstube.wix.com/home">other talents</a>.</p>
<p>I have read, or more precisely tried to read, <a href="https://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521719162">many</a> <a href="http://www.maths.ed.ac.uk/~aar/papers/maclanecat.pdf">books</a> <a href="http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf">about</a> <a href="http://www.mpi-sws.org/~dreyer/courses/catlogic/awodey.pdf">category</a> <a href="http://ebooks.library.cornell.edu/cgi/t/text/text-idx?c=math;cc=math;view=toc;subview=short;idno=Gold010">theory</a> but I know I still only have an intuition about very basic things, e.g. what is a category theory, composition of morphisms, units, simple limits… The more abstract - and important - concepts (adjunctions, Yoneda lemma, topoi, sheaves…) are still inaccessible to my understanding: To state it in Eugenia’s terms, I <em>know</em> them but I don’t <em>understand</em> them.</p>
<p><em>Cakes, Custard + Category Theory</em> is a math books for non-mathematicians, a book that tries and - to my humble opinion - somehow succeeds in giving lay people some ideas on why maths are important, interesting and fascinating. More importantly it also succeeds in giving <em>intuitions</em> on what <em>is</em> math and category theory and on connecting the dull, formal and painful external aspect of maths most people see with the deep, complex, beautiful and sophisticated ideas behind that rude shell.</p>
<p>The book is divided in two parts, <em>Mathematics</em> and <em>Category Theory</em> which shares a common underlying structure: Each small chapter is introduced by the <em>recipe</em> of some cake, some classical and some invented by the author, and each part ends with a tentative explanation of <em>what</em> is math or category theory. Through the various chapters, Eugenia threads mundane life examples, recipes, cooking metaphors with actual mathematical questions in order to convey to the reader intuitions on what things like monoids, groups, morphisms, associativity or equivalence are. Mathematical notations is mostly restricted to sidebars and does not clutter reading.</p>
<p>I am not usually a great fan of analogies which more often than not obscure rather than illuminate the thing they are supposed to explain. But this book is written in a such a way that it mostly avoids this pitfall<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>:</p>
<ul>
<li>Eugenia’s style is so lively and entertaining one cannot but avidly read the book and reach for the next chapter till the end,</li>
<li>She manages to <em>motivate</em> mathematics, not as a mere tool one uses to solve more or less complex problems, but as a way to ask profound questions about the way we think and the world we live in.</li>
</ul>
<p>The key question in maths and category theory is then not <em>how</em> but <em>why</em>.</p>
<blockquote>
<p>Proof has a sociological role ; illumination has a personal role. Proof is what convinces society ; illumination is what convinces us. In a way, mathematics is like an emotion, which can’t ever be described precisely in words - it’s something that happens inside an individual. What we write down is merely a language for communicating those ideas to others, in the hope they will be able to reconstruct the feeling in their own mind.</p>
</blockquote>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Although I myself enjoy cooking very much, I was not that much convinced about the whole recipe meme…<a href="#fnref1">↩</a></p></li>
</ol>
</div>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>A Personal Retrospective</title>
    <link href="http://abailly.github.io/posts/personal-retrospective.html" />
    <id>http://abailly.github.io/posts/personal-retrospective.html</id>
    <published>2016-04-12T00:00:00Z</published>
    <updated>2016-04-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>A Personal Retrospective</h1>
<div class="info">Posted on April 12, 2016</div>

<p>From August 2014 to April 2016 I have been working as CTO of a startup based in Singapore, developing a peer-to-peer lending platform using Haskell programming language and environment. This experience has now come to an end and this short blog post is a way for me to look back at those 20 months and reflect on the things I have learnt. I have some vague hopes this experience might be useful to others. But the main goal is for me to make explicit things that have stayed mostly implicit in order to ensure I can benefit from my own experience.</p>
<h1 id="haskell-rocks">Haskell Rocks</h1>
<p>Nothing new under the sun but working 120% of the time in Haskell over (nearly) the whole stack of our system really reinforced that belief. The point is, Haskell is not only awesome because of its own merits as a language and a platform. After all, Haskell is a language that is <a href="https://en.wikipedia.org/wiki/Haskell_%28programming_language%29#History">about 30 years old</a> and since its inception has mostly been confined in academic circles. This is apparent in some weaknesses in the build system, standard data types like Strings and numbers, lack of support for first-class modules…</p>
<p>Those deficiencies are compensated by a simple<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> yet powerful type-system, a vibrant eco-system of good to great libraries providing all the features one needs, a state-of-the-art compiler producing efficient programs, built-in support for powerful concurrency features… I think Gabriel Gonzalez’ <a href="http://www.haskellforall.com/2016/04/worst-practices-should-be-hard.html">Worst practices should be hard</a> offers some more solid arguments in favour of Haskell.</p>
<p>What sets Haskell apart is the fact that chosing to make it your core technology attracts interesting people. This is one of the effect mentionned by Paul Graham of using non-<a href="http://www.paulgraham.com/avg.html">Blub</a> languages: Not using a mainstream language attracts non-mainstream people. <a href="http://bos.github.io/strange-loop-2011/talk/talk.html">Bryan O’Sullivan</a> of<a href="http://book.realworldhaskell.org/">Real World Haskell</a> book fame also highlighted this point: In a world flooded by noise, Haskell acts as a signal.</p>
<p>This does not mean that there aren’t great people working in Java, Javascript or Php<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. And this does not mean I would like to work with any Haskeller just because she is a Haskeller. But from my experience hiring people, I found Haskell acted like an effective filter<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>: People who cared to answer job ads or reach out the company to enquire for job openings were more often than not <em>interesting</em>. They were more diverse<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> (different countries, different ages, different professional experiences…), more curious, more motivated and for those whom I had a chance to pair with, quite good at programming.</p>
<h1 id="tdd-rocks">TDD Rocks</h1>
<p>I have always been a strong proponent of <a href="/posts/tdd.html">Test Driven Development</a>. After all these years this practice is <a href="http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html">still</a> <a href="http://iansommerville.com/systems-software-and-technology/giving-up-on-test-first-development/">controversial</a> mostly because people conflate two different things: “writing regression tests” and “using tests to guide your design”, a confusion which is caused by the use of <em>Test</em> in Test-Driven Development. In a nutshell, one needs a very different mindset to jump from “I write tests to verify my program does what I intend it to do, now and in the future” to “I write executable specifications in order to ensure 1/ I understand the problem and 2/ my program does exactly what’s intended, no more, no less”.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> The former mindset usually leads to so-called white-box tests which are thorough but brittle and painful to maintain.</p>
<p>Even within the Haskell community TDD is not widely accepted as a standard practice of development. I personally think Haskell is a really great fit for TDD if you take it in a broad sense, that is if you include all the tools the language and platform provide as part of your TDD cycle. This means leveraging the interpreter, compiler, type-checker and test runner to shorten the feedback loop.</p>
<p>Maintaining tests require (a lot of) effort and discipline, and it is tempting when pressed to deliver to cut corners. And I sometimes have done it myself: Comment out an flaky test in order to stabilize build. But except in one occasion, I have always come back to it later and reinstate it. These efforts really pay off in the long run, when you start adding features upon features, when the complexity of the system becomes too important to “fit in your head”, when you need to modify existing features or extend them to cope with new behaviours. TDD gives you both a safety net - so that it breaks when you change your code and highlights other parts of the system that need to be adapted - and a guide on current behaviour of the code.</p>
<h1 id="remote-development-works">Remote Development Works</h1>
<p>Although the company’s business is exclusively located in Singapore I never lived there and we started working remotely, me in France and my partner in Singapore. The development team has been distributed for almost all of the past 20 months, with people in England, Portugal, India, Poland and Malaysia (not all at the same time). And we managed to develop a platform that handles a modest load but has been working reliably managing financial and personal data since March 2015, steadily adding features, fixing bugs, deploying new versions several times a week or even a day, building and maintaining build and production infrastructure…</p>
<p>We used some simple form of agile methodology with daily “standup” meetings that allowed us to talk to each other at least once a day, strict automated tests, continuous integration and frequent releases. This made it possible to have rapid feedback from business people even if I was not sitting in the same room most of the time. We exposed our work process through Trello, used communication tools like Slack, Hangout, Skype, and tried a few others, and we managed to build a consensus across the whole development team on what was going on and what we had to do. We even manage to <a href="https://pragprog.com/book/jkrp/remote-pairing">pair program</a> on a somewhat regular basis.</p>
<p>As already advocated in <a href="https://37signals.com/remote">Remote</a> book, working remotely works under some conditions:</p>
<ul>
<li><em>Distribute whole team</em>: Having most of the team colocated with one or two persons distributed does not work,</li>
<li><em>Trust the people</em>: You have to trust each other and assume everybody is doing his or her best,</li>
<li><em>Communicate constantly</em>: You have to be very explicit about what you are doing, even if working alone, and you have to constantly try to detect and solve potential conflicts, misinterpretations, misunderstandings that could quickly degenerate,</li>
<li><em>Use the right tools</em>: Emails are a useful tool but one which is often abused<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>, we need “hotter” media like chat, video/phone…</li>
</ul>
<p><a href="https://open.buffer.com/">Buffer</a> is a good example of a company that has chosen to be fully distributed and is very transparent on how it works on a daily basis.</p>
<h1 id="but-it-needs-energy">… But it needs energy</h1>
<p>While keeping in touch with development team was always easy, doing same on the business side quickly became very hard. This is of course related to the very localized nature of the business the company was doing, but also to different background and maturity with respect to remote working. Remote working is definitely a viable option for software development as the success of a lot of world-spanning open-source projects has demonstrated.</p>
<p>As I advocated above, working effectively as a remote team needs some requirements to be met. But even if those requirements are met, it still can fail if people are not trained and do not make the mental leap to make it work. It might be possible that developers, having to deal constantly with abstractions, networks, virtualities, are more prone to make that leap. Once you consider it normal to work on machines located in a data center 10000 kms away, it is a small feat to consider normal to work with another developer located 10000 kms away. The network becomes an extension to standard Earth geography and there is a form of excitement in the way modern technology allows us to break distance barriers<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>.</p>
<p>Unfortunately, this particular mindset is not widespread among people practicing other trades. “Business people” who don’t need to interact constantly with developers quickly lose grasp and stop putting energy in maintaining a communication link that’s not obvious to them. The whole zoo of tools we are using appears daunting when compared with the simplicity of Outlook and Excel. When one has a lot more face-to-face interactions than online ones, she or he is quite prone to drop the latter in favour of the former. Note that working as a distributed team is <em>not</em> to be equated with working <em>remotely</em>. Obviously, organizations have been distributed for a long time: Businesses are used to employ people like salespersons who largely work remotely or to have various business units all over the planet. But this is different from the kind of real-time cooperation and interactions one needs when developing software and working as a distributed team.</p>
<h1 id="takeaways">Takeaways</h1>
<p>You can’t summarize 20 months of your life in a couple of bullet points and I definitely think this experience was amazing and has changed my life and the way I envision my work in a very deep way:</p>
<ul>
<li>Working as team remotely can be both satisfying and efficient when done properly,</li>
<li>Working with people from diverse origins and nationalities in a foreign setting is exciting<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>, and travelling to work with those people occasionally is the best way to discover local culture,</li>
<li>Haskell is <em>really</em> practical for large-ish systems development.</li>
</ul>
<p><strong>Afterwords</strong>: Many thanks to Cédric and Éric for their feedback on this small piece.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Yes, simple when compared with something like Scala’s or C++ type system. Haskell’s type system only uses a few key concepts that can be rather easily understood and explained and which allow you to build powerful abstractions on top of it without resorting to syntax-directed tricks (e.g. macros).<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I personally know quite a few of them…<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Of course, there is still the possibility this was just another way of selecting like minded people thus implicitly rejecting genuine <em>diversity</em><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>With the notable exception of gender diversity: Over the 50+ person I interviewed I have had a single woman.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>For a good rebutal of the previous arguments, see <a href="http://blog.cleancoder.com/uncle-bob/2016/03/19/GivingUpOnTDD.html">Uncle Bob</a>’s reply to the “Giving up on TDD” post.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Emails are terrible for discussions - exchanging and/or arguing over some more or less complex point - or task tracking - maintaining and updating status of some work in progress - yet they are unfortunately often used that way.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>As people dealing with technology we might also be keener to fall to its trap and seductive power.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Having to make yourself understood in a language (English) which is not the mother tongue of any of the people you work with is sometimes frustrating, but always interesting. In can provide some natural dampening of feelings and emotions that cannot fail to crop up in any collective endeavour.<a href="#fnref8">↩</a></p></li>
</ol>
</div>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>Anatomy of a Haskell-based Application</title>
    <link href="http://abailly.github.io/posts/cm-arch-design.html" />
    <id>http://abailly.github.io/posts/cm-arch-design.html</id>
    <published>2015-11-16T00:00:00Z</published>
    <updated>2015-11-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>Anatomy of a Haskell-based Application</h1>
<div class="info">Posted on November 16, 2015</div>

<p>This is the first post of a series I am planning to write about my experience developing software as CTO of <a href="http://www.capital-match.com">Capital Match</a>, a Singapore-based startup providing a peer-to-peer lending marketplace for Small and Medium Businesses and private and corporate investor.</p>
<p>This post is about the design and architecture of the system itself, the choices and tradeoffs that were made, whether good or bad. In the conclusion I try to provide an assessment of the current situation and reflect on those choices.</p>
<h1 id="fundamental-design-choices">Fundamental Design Choices</h1>
<h2 id="haskell">Haskell</h2>
<p>Basing Capital Match’s tech stack on Haskell was an obvious choice for me from the onset, even if I had had limited professional experience with Haskell before that:</p>
<ul>
<li>Haskell is a <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/">very mature language</a> with a <a href="https://www.haskell.org/ghc/">state-of-the-art compiler</a> that receives constant attention from a bunch of extremely bright people and thus keeps improving and evolving,</li>
<li>Haskell’s tools for building robust web-based applications is not as mature as what you can find in Java or .Net worlds but it is evolving quickly as the platform is gaining traction thanks to efforts from both a vibrant <a href="https://wiki.haskell.org/Haskell_Communities_and_Activities_Report">community</a> and few but dedicated <a href="https://github.com/commercialhaskell">private bodies</a>,</li>
<li>Haskell developers are few and far between, but their number is growing and they are more often than not passionate and talented,</li>
<li>I have been programming for fun and small side projects in Haskell since 2002 and I always have wanted to know how it would feel to build a whole system with it. Now I know,</li>
<li>Because it enforces a strict separation of pure and effectful code, Haskell incentivizes the growth of a <a href="http://alistair.cockburn.us/Hexagonal+architecture">Hexagonal Architectures</a> aka. <a href="http://c2.com/cgi/wiki?PortsAndAdaptersArchitecture">Ports and adapter</a>: A pure domain kernel which interacts with the outside world through <em>adapters</em>.</li>
</ul>
<h2 id="event-sourcing">Event Sourcing</h2>
<p>The system was designed from the onset as an <a href="http://martinfowler.com/eaaDev/EventSourcing.html">event-sourced</a> application: The source of truth in the system is a sequence of <em>events</em> where each event defines a transition between two states. At any point in time, the state of the system is thus whatever state the current sequence of events leads to. Among the motivations behind using ES are:</p>
<ul>
<li>Having fun and explore this corner of the design space instead of going for the more traditional RDBMS-based web app,</li>
<li>Auditability and traceability of all actions impacting data on the platform, a property which is highly-desirable in a banking-like system. I have had previous exposure to finance software and they all end up implementing some journalling system to trace users actions and data changes,</li>
<li>Reluctance to add the operational burden of maintaining a RDBMS as part of the system. We could have used SaaS relational (or non-relational) database to remove that burden but this implies using yet another tool, learning some other piece of technology, using some set of drivers with specific bugs and requirements,</li>
<li>Personal bias against RDBMS used as runtime storage<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>,</li>
<li>Simplicity of implementation, at least when you don’t require HA, partition tolerance or more generally fault-tolerance: A single file to which all events are appended is enough, and this is exactly what we do,</li>
<li>Avoiding languages impedance mismatch. There is the tradional <a href="http://c2.com/cgi/wiki?ObjectRelationalImpedanceMismatch">Object-relational Impedance Mismatch</a> although <a href="http://blog.jooq.org/2015/08/26/there-is-no-such-thing-as-object-relational-impedance-mismatch/">some have argued</a> it is not where we usually think it is. As argued in the latter I think the real issue is in SQL: SQL is (probably) great for writing complex queries<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> but not so much for inserting data.</li>
</ul>
<h1 id="architecture">Architecture</h1>
<p>The main interface to the system is a REST-like API providing various resources and actions over those resources. Most exchanges with the outside world are done using JSON representation of resources, with some CSV. The <em>User Interface</em> is merely a client of the API and is (morally if not to the letter) a single page application. There is also a command-line client which offers access to the complete API and is used for administrative purpose.</p>
<h2 id="models">Models</h2>
<p>The core of the application is purely functional and made of several loosely coupled <code>BusinessModel</code> instances (think Aggregates in <a href="https://en.wikipedia.org/wiki/Domain-driven_design">DDD</a> parlance) that each manage a specific sub-domain: <code>Accounting</code> manages accounts and transactions, <code>Facility</code> manages facilities lifecycle, <code>Investor</code> and <code>Borrower</code> manage profiles and roles-dependent data, <code>User</code> manages basic registration, authentication and settings for users (e.g. features)…</p>
<p>A <code>BusinessModel</code> is defined as:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">BusinessModel</span> a <span class="kw">where</span>
  <span class="kw">data</span> <span class="dt">Event</span><span class="ot"> a ::</span> <span class="fu">*</span>
  <span class="kw">data</span> <span class="dt">Command</span><span class="ot"> a ::</span> <span class="fu">*</span>
<span class="ot">  init ::</span> a
<span class="ot">  act ::</span> <span class="dt">Command</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">Event</span> a
<span class="ot">  apply ::</span> <span class="dt">Event</span> a <span class="ot">-&gt;</span> a  <span class="ot">-&gt;</span> a</code></pre></div>
<ul>
<li>A type of events this model generates</li>
<li>A type of commands this model can process</li>
<li>An initial value for the model</li>
<li>A pair of functions describing which event is generated by a command <em>acting</em> on the model and how an event changes the model when it is <em>applied</em> to it</li>
</ul>
<p>The state of each BusinessModel instance is computed upon application startup by loading all the events and applying each stored event to an <code>init</code>ialised model. Models are then kept in memory while events are stored persistently. This initial startup process takes a couple of seconds given the small scale at which we operate.</p>
<p>Each model demarcates transactional boundaries and is the unit of consistency within the system. Commands and events on a single model are assumed to occur <em>sequentially</em>.</p>
<h2 id="services">Services</h2>
<p>Above <code>BusinessModel</code>s are <code>Service</code>s which provides the interface to the system. Services orchestrate the interactions of one or more Models. At the simplest level, a <code>Service</code> simply consists in the application of a single <code>Command</code> on some <code>BusinessModel</code>, but it can be more complex, synchronizing application of commands over several models. Based on the ideas exposed in <a href="http://adrianmarriott.net/logosroot/papers/LifeBeyondTxns.pdf">Life Beyond Distributed Transactions</a>, a <code>Service</code> represents the state of the interaction between a single user of the system, e.g. a request, and one or more piece of data maintained by the system.</p>
<p>Because they are the agents of the outside world in the system, <code>Service</code>s operates in an impure context, hence in a dedicated <code>Monad</code> called <code>WebM</code>. Services typically return some representable type, when they are queries, or an <code>Event</code> denoting the outcome of the request. <code>WebM</code> is actually an instance of a monad transformer <code>WebStateM</code> over IO, hence it is impure. It has access to 2 pieces of state.</p>
<p>Here is the definition of <code>WebStateM</code>:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">newtype</span> <span class="dt">WebStateM</span> shared local m a <span class="fu">=</span> <span class="dt">WebStateM</span> {<span class="ot"> runWebM ::</span> <span class="dt">TVar</span> shared <span class="ot">-&gt;</span> local <span class="ot">-&gt;</span> m a }

<span class="kw">type</span> <span class="dt">WebM</span> a <span class="fu">=</span> forall s <span class="fu">.</span> <span class="dt">EventStore</span> s <span class="ot">=&gt;</span> <span class="dt">WebStateM</span> <span class="dt">SharedState</span> <span class="dt">LocalState</span> s a</code></pre></div>
<p>This is simply a <code>Reader</code> monad with two different pieces of data:</p>
<ul>
<li><code>LocalState</code> is filled with information relevant to a single query (e.g. user id, request id, time…),</li>
<li><code>SharedState</code> is a <code>TVar</code> (transaction variable living in <code>STM</code> monad) that is shared across all requests,</li>
<li>The <code>EventStore</code> constraint means we need the underlying monad to provide access to persistent storage.</li>
</ul>
<p>The vast majority of services use the generic <code>applyCommand</code> function which is the critical part of the system. This function is responsible for:</p>
<ul>
<li>applying the command and updating the stored Model,</li>
<li>persist the event in the “database”,</li>
<li>dispatch the event to interested components.</li>
</ul>
<h2 id="web">Web</h2>
<p>The REST interface is provided by <a href="https://github.com/scotty-web/scotty">scotty</a> which is a simple framework based on <a href="https://github.com/yesodweb/wai">WAI</a> and <a href="https://github.com/yesodweb/wai">warp</a><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. Most action handlers are pretty simple:</p>
<ul>
<li>They extract some parameters or JSON data from the body of the request,</li>
<li>They invoke some service,</li>
<li>They provide an HTTP response according to the result returned:
<ul>
<li>Queries simply serialize the result to JSON or other requested media type,</li>
<li>Actions look at the returned <code>Event</code> to provide meaningful answers.</li>
</ul></li>
</ul>
<p>On top of REST endpoints sit some <code>Middleware</code>s which check or apply transformations to requests and/or responses:</p>
<ul>
<li>Provide a <code>Request-Id</code> header,</li>
<li>Authorisation and authentication,</li>
<li>Sanity checks (e.g. sizes of payloads),</li>
<li>Logging,</li>
<li>Caching and static data service.</li>
</ul>
<h2 id="lost-in-translation">Lost in Translation</h2>
<p>Executing a user-triggered action is in a sense a series of translations occuring between different <em>level of languages</em>:</p>
<ul>
<li>From REST to <code>WebM</code> we use <code>inWeb :: WebStateM CapitalMatchState LocalState m a -&gt; ActionT e (WebStateM CapitalMatchState   LocalState m) a</code>,</li>
<li>From <code>WebM</code> to <code>STM Model</code> we use <code>liftIO . atomically</code>,</li>
<li>… and finally in <code>Model</code> we reach the pure kernel of the business domain!</li>
</ul>
<p>Conceptually, we have this hierarchy of monads, expressed in types:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">model ::</span> <span class="dt">Command</span> <span class="ot">-&gt;</span> <span class="dt">StateT</span> <span class="dt">STM</span> <span class="dt">Model</span> (<span class="dt">Event</span> <span class="dt">Model</span>)
<span class="ot">service ::</span> <span class="dt">WebM</span> (<span class="dt">Event</span> <span class="dt">Model</span>)
<span class="ot">web ::</span> <span class="dt">ActionT</span> ()</code></pre></div>
<p>This hierarchy of monads delineates, somewhat obviously, the following languages:</p>
<ul>
<li>Language of <em>Models</em> expresses atomic (e.g. often CRUDesque) changes to a Model, like <code>RegisterTransaction</code>, <code>UpdateProfile</code> or <code>CloseFacility</code>,</li>
<li>Language of <em>Services</em> expresses either direct changes to Models or more complex interactions like <code>addPledge</code> or <code>acceptFacility</code> which require more than one command to complete,</li>
<li>Language of <em>Web</em> manages HTTP Requests and responses, JSON structures and delegate work to services. It is the language of representation of things.</li>
</ul>
<h1 id="cross-cutting-concerns">Cross-cutting Concerns</h1>
<h2 id="concurrency">Concurrency</h2>
<p>Concurrency is mostly handled at the REST layer through Warp and Scotty: Each request is handled concurrently by separate threads which are <a href="https://ghc.haskell.org/trac/ghc/wiki/LightweightConcurrency">very lightweight in Haskell/GHC</a>. On top of that we have a couple more threads in the application:</p>
<ul>
<li>One logging thread per handler (currently 2), which handle logging messages,</li>
<li>A storage thread which handle low-level read/write requests to events file,</li>
<li>A driver thread which handle events storage<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>,</li>
<li>A <em>Heartbeat</em> thread periodically checks other threads and notifies health.</li>
</ul>
<p>We used to run directly threads with <code>forkIO</code> and friends but finally moved to something simpler and much more robust: The <a href="http://hackage.org/packages/async">async</a> package. Concurrent updates to the model are handled through <a href="http://hackage.org/packages/stm">Software Transactional Memory</a>: A <code>TVar</code> (transactional variable) holds a reference to the state and all operations on the state are thus transactional.</p>
<p>The initial goal was to enforce a strict separation between the various <em>Business Models</em> with an eye towards being able to deploy them as independent services exchange messages. But it happened this rule was broken more than once and a few months later we ended up having built a monolith with uncontrolled dependencies across domains and layers. We then undertook the necessary refactoring steps to get back to a “saner” state where <code>STM</code> transactions operate at the level of a single <code>Command</code> through the <code>applyCommand</code> function.</p>
<h2 id="persistence-and-storage">Persistence and Storage</h2>
<p>Persistence is managed through a dedicated event bus: <code>Event</code> are first packaged into a more opaque <code>StoredEvent</code> object containing metadata useful for traceability of the system:</p>
<ul>
<li>An event version (more on this later)</li>
<li>An event type which encodes at the value level the actual type of event</li>
<li>Timestamp (in UTC)</li>
<li>ID of user generating the event</li>
<li>ID of original request</li>
<li>SHA1 of commit, e.g. version of the code (thanks to <a href="http://geoffroycouprie.com/">Geoffroy Couprie</a> for the suggestion)</li>
<li><code>ByteString</code> payload containing serialized version of the event</li>
</ul>
<p>Then <code>StoredEvent</code>s are pushed to a dedicated <code>Driver</code> thread which stores events in the underlying events file. Physical <code>Storage</code> is a simple append-only file which contains sequence of applied events serialized to some binary format (<a href="http://kafka.apache.org">Kafka</a>-like). We are in the process of moving to a much more robust storage solution:</p>
<ul>
<li>externalize data store to another process/host,</li>
<li>replicate it to increase fault-tolerance,</li>
<li>provide strong consistency through distributed consensus.</li>
</ul>
<h3 id="event-versioning">Event Versioning</h3>
<p>Events are stored with a <em>version</em> number which is a monotonically increasing number. This version number is bumped each time we change the structure of our events, e.g. adding a field to some record, changing a field’s type… When an event is persisted, it contains the <em>current version</em> number that’s defined in the application at that time. When the event is read back (i.e. deserialized) to rebuild the state of the system, this version number is used to select the correct read function.</p>
<p>Hence modifying the structure of events always entails the following steps in development:</p>
<ul>
<li>Write deserialization test for current version number<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>,</li>
<li>Bump version number,</li>
<li>Write new code,</li>
<li>Write <em>migration code</em> to adapt old events to new version.</li>
</ul>
<p>This mechanism adds some interesting properties to our underlying storage:</p>
<ul>
<li>Stored events are immutable hence storage system is append-only: We never need to rewrite past events,</li>
<li>It is possible to rebuilt state of the system (code <em>and</em> data) at any point in the past.</li>
</ul>
<h2 id="user-interface">User Interface</h2>
<p>UI code still lives partly in the server and partly as pure client-side code:</p>
<ul>
<li>HTML code is generated and served by the server using standard endpoints according to <code>Accept</code> header in request. We use <a href="https://hackage.haskell.org/package/blaze-html">blaze-html</a> combinators to describe the pages in Haskell,</li>
<li>Static assets are served by <a href="https://hackage.haskell.org/package/wai-middleware-static">wai-middleware-static</a></li>
</ul>
<p>But the grunt of UI work is done on the client with <a href="https://github.com/omcljs/om/">Om</a>. Om is a <a href="http://github.com/clojure/clojurescript">clojurescript</a> interface to Facebook’s <a href="http://facebook.github.io/react/">React</a><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>. We treat the UI-backend interaction as a pure client-server: UI maintains its own state and interact with server through usual Ajax calls, updating state accordingly. The interface a user sees is a single-page application.</p>
<h2 id="logging-and-monitoring">Logging and Monitoring</h2>
<p>There is a <code>Log</code> structure which is a queue consuming logging events and handling it according to some function. We log all queries, all commands issued and various other events occuring in the system: application startup/stop, heartbeat, I/O errors, storage events… In order to prevent sensitive data to leak to logging, we have a <code>redact</code> function that rewrites commands to remove those data before passing it to logging system.</p>
<p>We currently have two different log backends:</p>
<ul>
<li>One log handler outputs JSON-formatted events to <code>stdout</code>,</li>
<li>One log handler outputs some events to <a href="http://riemann.io">Riemann</a>, using <a href="https://github.com/tel/riemann-hs">riemann-hs</a>. Those events are then used for notification and monitoring of infrastructure (more on this in a later post).</li>
</ul>
<p>At startup of application we also notify dev team by sending an email with the configuration. This is useful to check startup of production environment, as this email contains among other things the version of the application and the command line with which is has been started.</p>
<h1 id="reflection">Reflection</h1>
<p>It’s been a bit over a year since I have started working on Capital Match’s platform. I – we – have made mistakes, not everything went as smoothly as we would have liked and it is still just the beginning of a hopefully long adventure. One year is a good time to stop - or slow down a bit - and reflect on what’s been accomplished, what went wrong and what went well. In the next sections I try to provide a more or less objective assessment of the architecture we have put in place, and what would be our next steps.</p>
<h2 id="the-good-the-bad-and-the-ugly">The Good, the Bad and the Ugly</h2>
<p>We have been live since March 2015, serving more than S$ 3 millions - and counting - in facilities for SMEs in Singapore without any major interruption of service. This in itself is an extremely positive fact: It works and it supports continuous improvements in a smooth way<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>.</p>
<p>Here are some benefits I see in our approach, encompassing both the technology used (Haskell, Om/Clojurescript) and the architecture:</p>
<ul>
<li>Strong and expressive types greatly improves confidence in the code<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>. Among the <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghc-language-features.html">many features</a> supported by GHC, here are the ones, mostly simple and straightforward, we use routinely:
<ul>
<li><code>newtype</code>s are cheap in Haskell, being unpacked at runtime, but they are enforced during compilation and makes for very expressive signatures: No more <code>String</code>-based programs but <code>UserId</code>, <code>PassportNr</code> or <code>EMail</code>,</li>
<li>Phantom types is a simple technique to distinguish between various objects with equivalent representations, like encoding of a <code>ByteString</code>,</li>
<li>Type-classes are very useful to define interfaces, possibly with default implementations. Most of the time we use <code>MultiParamTypeClasses</code> to express relations between different part of the systems,</li>
<li>Existential types are useful to <em>pack</em> related things under a common opaque type for e.g. serialization or logging, where you don’t care about the details,</li>
<li>But most of the time using a simple type with a set of constructors is clearer.</li>
</ul></li>
<li>We use <code>-Wall -Werror</code> for compiling code which catches things like unused variables (dead code), variable names overriding (potential troubles), incomplete pattern matching (runtime failure ahead…),</li>
<li>Event Sourcing greatly simplifies storage management and removes all the hassle of having to manage data types mapping to a relational model, not speaking of managing migration between versions or the burden of operating a DBMS,</li>
<li>Because data is stored as a flat file of versioned events, querying the system can be done directly in Haskell: Retrieve event stream, build in-memory state from it then use GHCi or Emacs REPL to manipulate state<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> at your will. Over time we have written a number of <em>scripts</em> that are small Haskell programs implementing complex queries (e.g. user funnel) or programmatic one-shot transformations of data,</li>
<li>Scotty, WAI and Warp makes it easy to quickly develop and maintain REST interfaces, both for endpoints and middlewares.</li>
</ul>
<p>Here are some mistakes I made:</p>
<ul>
<li>Too many use of partial functions like <code>fromJust</code> or even <code>head</code>: This makes things simple at first but of course blows up at runtime. Lesson learned: Always use <strong>total functions</strong>,</li>
<li>Too many typeclasses: This might come from my Java background where using an interface is usually a good idea to abstract details. Over the time, we have created new typeclasses to answer to express new behaviour, which clutters types,</li>
<li>Not taking enough care to keep compilation time low: Our application has grown over time, and I have not taken care of splitting it early enough to prevent bloat. Compilation time has grown over time to the point where it is now a problem. Lesson learned: Aggressively split code as early as possible, and don’t be afraid of having packages with one or two files,</li>
<li>Too much reliance on <code>DeriveGeneric</code> based JSON serialization: Generating <code>Generic</code> instance for large types dramatically increases compilation time. Lesson learnt: Use more <code>TemplateHaskell</code> derivation or custom <code>ToJSON/FromJSON</code> instances which provide better flexibility<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></li>
<li>Having data accessible only through Haskell implies non-tech people either need to learn it or go through the dev team to get data. This is not too much of a trouble in a very small team but could quickly become a problem as the company grows. This is where <a href="http://martinfowler.com/bliki/CQRS.html">CQRS</a> will nicely complement our Event Sourced system: It is a rather simple matter to build one or more relational models from our data and ensure the RDBMS is updated on a regular basis,</li>
<li>Separation of concerns among the various business models has been sloppier over time, leading to too much coupling among the models,</li>
<li>Events are not invertible which means that one cannot travel back and forth in an event stream easily to select required state,</li>
<li>Generating HTML on the server-side: At the onset of the project, we had a static HTML file. We moved to server-side HTML generation using Blaze because we wanted to be able to control the structure of the HTML:
<ul>
<li>To cope for dev/prod environment on the front-end: Code is compiled and optimized differently in the two modes and this requires importing a different set of scripts,</li>
<li>To manage <em>feature toggles</em> which allow us to provide a different UI for different users according to their account’s settings. This was very useful to handle gracefully migration of our UI,</li>
<li>However this strategy entails a number of problems:
<ul>
<li>You sometimes have to recompile server when working on the UI, e.g. when changing structure of pages or handling of features,</li>
<li>It’s hard to collaborate with front-end designers and developers,</li>
<li>It makes UI and server more coupled,</li>
</ul></li>
</ul></li>
<li>REST interface seems to be growing too quickly. There are some query abstractions that should be developed out of the raw endpoints we are exposing.</li>
</ul>
<h2 id="whats-next">What’s Next?</h2>
<p>Within the span of a single year, much has happened in the Haskell ecosystem and things that were experimental or unwieldy one year ago are now mature and could easily make their way to production: <a href="https://github.com/ghcjs/ghcjs">ghcjs</a> is now much easier to build and work with, there are more mature solutions in the front-end like <a href="https://github.com/ryantrinkle/reflex">reflex</a>, build has never been easier thanks to <a href="https://github.com/commercialhaskell/stack/">stack</a>, <a href="https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/release-7-10-1.html">GHC 7.10</a> has brought a number of improvements (and controversial breaking changes like the TAP proposal)… Gabriel Gonzalez maintains a <a href="http://www.haskellforall.com/2015/08/state-of-haskell-ecosystem-august-2015.html">State of Haskell Ecosystem</a> page that provides interesting overview of what’s hot and what’s not in the Haskell world.</p>
<p>Here are some major challenges that lie ahead of us to improve our system:</p>
<ul>
<li><strong>Services</strong>: In spite of good initial intention we still have built a monolith, albeit a small one and one that will be not too hard to split. We now want to increase robustness, resilience and scalability of our development process and our system by breaking the monolith into components services. We are in the process of splitting the application into smaller constituents along the following lines:</li>
<li>Glue code to wire things together in a single app,</li>
<li>Support code,</li>
<li>One component per group of related services,</li>
<li>One component per “Model”, possibly clustered,</li>
<li>Ideally each component should be deployable independently or alongside other components in the same process depending on needed granularity and redundancy. This can be achieved easily through configuration at the level of the glue code according to some topology configuration.</li>
<li><strong>Performance</strong>: I tried to follow this simple development principle: <a href="http://c2.com/cgi/wiki?MakeItWorkMakeItRightMakeItFast">Make it, make it right, make it fast</a>. We mostly are done with the first part and are quite advanced on the second, so making it fast will be our next challenge especially as user base and data set grow in size. There are quite a few areas of improvement on the front: Caching computations, better data structures, improve strictness in key areas… But of course the first step will be to measure and set goals for those performance improvements.</li>
<li><strong>Robustness</strong>: No system is ever safe from failure but it depends on us what the impact of a failure is. This is definitely a must-have and something we can improve using standard replication and redundancy techniques. Splitting the system in finer-grained components is a first step towards that goal but we need specific components to ensure consistency in presence of failure.</li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>This article is already quite long yet it is only a brief overview of our system: It is hard to summarize one year of intense work! In future installments of this blog post series I plan to address other aspects of the system that were not covered here: Development and production infrastructure, user interface development from the point of view of a backend developer, development process.</p>
<p>As a final note, my highest gratitude goes to the following persons without the help of whom this adventure would not have been possible: Pawel Kuznicki, Chun Dong Chau, Pete Bonee, Willem van den Ende, Carlos Cunha, Guo Liang “Sark” Oon, Amar Potghan, Konrad Tomaszewski and all the great people at <a href="http://www.capital-match.com">Capital Match</a>. I also would like to thank Corentin Roux-dit-Buisson, Neil Mitchell, Joey Hess for their support and feedback.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I have been using RDBMS since the 90’s, developed a point-of-sale application in Access, have been using PostgreSQL through its various versions since 1998, and recently worked on integrating DB migration process into a very large system. I am not an expert but I have had quite an extensive experience of relational databases over a significant number of years and I have always found that <em>writing</em> to DB quickly became a painful things.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Although one could argue that there exists “languages” like Excel that allow you to write complex queries and explore data in a very sophisticated way without the use of SQL<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="http://haskell-servant.github.io/">Servant</a> is definitely on our roadmap.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This thread is pretty much redundant with storage thread for the moment. The plan is to use it for serialising <code>applyCommand</code> operations on the models<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>We use <a href="https://hackage.haskell.org/package/QuickCheck">QuickCheck</a> to generate a bunch of events for the type of interest.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>It looks like our Om will be soon superceded by <a href="https://github.com/omcljs/om/wiki/Quick-Start-%28om.next%29">om.next</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>I plan to provide more insights on our development and operations process in another blog post, but to give rough ideas we have deployed our application about a hundred times in the past 6 months.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Having a strong type system is no replacement for a decent test suite however, because obviously a lot of bugs happen at the boundaries of the system, e.g. when invoking REST API.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Obviously, this works as long as your data fits in memory. My bet is this will be the case for <a href="http://yourdatafitsinram.com/">quite a long time</a>. Shall this ever become a <a href="https://gettingreal.37signals.com/ch04_Scale_Later.php">problem</a>, we will most probably be in a position to handle it.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>This is the approach I took in <a href="https://github.com/capital-match/hdo/blob/master/src/Network/DO/Types.hs">hdo</a> because external representation was already defined, and in the end it makes encoding more explicit and easier to work with<a href="#fnref10">↩</a></p></li>
</ol>
</div>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>Sur &quot;Du mode d&#39;existence des objets techniques&quot;</title>
    <link href="http://abailly.github.io/posts/objets-techniques.html" />
    <id>http://abailly.github.io/posts/objets-techniques.html</id>
    <published>2015-11-07T00:00:00Z</published>
    <updated>2015-11-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>Sur &quot;Du mode d&#39;existence des objets techniques&quot;</h1>
<div class="info">Posted on November  7, 2015</div>

<p>Je suis un grand <a href="/posts/eme.html">admirateur</a> de Bruno Latour, même si je ne suis pas certain de comprendre l’ensemble des concepts qu’il développe. Mais il y a une chose que je crois comprendre et qui m’attire dans sa pensée, c’est l’idée, développée en particulier dans <a href="">Nous n’avons jamais été moderne</a> et bien sûr <a href="">Enquête sur les modes d’existence</a> que Nature et Culture ne sont pas deux blocs antagonistes mais deux pôles en interaction dont les sciences et les techniques sont, entre autres, des médiateurs dont il faut se préoccuper. Dans EME il identifie un certains nombres d’êtres et de modes d’existence d’iceux. Latour cite régulièrement <a href="http://gilbert.simondon.fr/">Gilbert Simondon</a> et lui emprunte d’ailleurs une partie du titre de sa thèse: <a href="http://editions.flammarion.com/Albums_Detail.cfm?ID=41344&amp;levelCode=home">Du mode d’existence des objets techniques</a>.</p>
<p>Simondon est un des rares philosophes qui essaye de <em>penser</em> la technique, ses objets et sa relation à l’homme. Son livre est dense, ardu, difficile à lire avec un style hâché et une profusion de termes peu usités et de tournures de phrases quelques peu alambiquées, je ne suis pas sûr d’en avoir saisi toutes les subtilités n’ayant pas eu la possibilité de fréquenter la rue d’Ulm mais j’en ai retenu quelques éléments qui résonnent avec mon expérience de programmeur et mainteneur de logiciels.</p>
<p>Un <em>objet</em> technique est un <em>individu</em> technique issu par <em>invention</em> d’un assemblage inédit d’<em>éléments</em> techniques, constituants atomiques de la <em>technicité</em> dans lesquels résident une puissance, une capacité d’agir sur le réel et qui sont le produit d’un <em>ensemble technique</em>. Il n’y a pas de hiérarchie dans la technicité entre éléments, objets et ensembles. Un marteau par exemple, est un objet technique issue de l’assemblage d’éléments pré-existants : une masse métallique et un manche en bois pour faire simple ; le marteau s’insère dans un ensemble technique, p.ex. la menuiserie ou même plus généralement la construction, les métiers du bâtiment, dans lequel il joue un rôle particulier. La fonction de l’objet technique n’est pas fixée par sa forme mais dépend de son milieu, mais il subit un processus d’<em>adaptation</em> lorsqu’inséré dans un milieu il se modifie par l’usage qui est en fait : la forme et la fonction sont reliés l’une à l’autre par une boucle de rétroaction sans qu’un des éléments soit prépondérant.</p>
<p>Simondon développe le concept d’abstraction et de concrétisation de l’objet technique pour décrire son évolution. Un objet technique sera d’autant plus concret qu’il sera “adhérent” au monde et que ses éléments seront en interaction “harmonieuses”, et par conséquent d’autant plus abstrait qu’il sera détaché du monde, générique. La concrétisation exprime le degré de perfection d’un élément ou d’un objet technique. Ce qui est particulier à l’objet c’est que ses éléments constitutifs peuvent produire des contraintes antagonistes, contraintes qui peuvent se résoudre soit en réduisant la marge d’indétermination de l’objet dans son usage<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, soit au travers de l’invention et de la découverte dans un changement de l’un ou de plusieurs des éléments de l’objet, voire carrément dans la création d’un nouvel objet. Simondon développe l’exemple du moteur à explosion dont le corps subit les contraintes de résistance à l’explosion et de dissapation de la chaleur: l’invention d’ailettes perpendiculaires à la paroi permet de résoudre partiellement les deux contraintes, mais alors une nouvelle tension apparaît entre le volume des ailettes et celui de la soupape.</p>
<p>Les objets techniques ont donc une sorte de logique interne, une existence propre relativement indépendante de leur fonction et qui résulte d’une tension entre des contraintes physiques, des éléments, entre matière et forme indisolublement liés dans l’objet. L’objet technique n’est pas que fonction mais aussi forme, assemblage d’éléments et constituant d’ensembles. L’ensemble technique est le stade “moderne” d’évolution de la technicité, et ce qui appelle une <em>technologie</em>. Le rejet de la technique par l’homme apparaît quand l’homme devient lui-même élément d’un objet technique - la <em>machine</em> - et seul le passage à l’étape suivante c’est-à-dire la compréhension des ensembles et l’insertion de l’humain dans ces ensembles permet de dépasser cette opposition en faisant de la technique une part intégrale de la <em>culture</em>.</p>
<p>Dans la deuxième partie du livre Simondon propose une explication génétique du fait technique et de ce qu’il engendre dans la société humaine basée sur l’opposition entre <em>figure</em> et <em>fond</em>, une dualité qui vise à supplanter la dualité traditionnelle entre <em>forme</em> et <em>matière</em><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>:</p>
<ul>
<li>dans les temps immémoriaux, aucune séparation n’existe: l’humain ne se distingue pas de la nature dont il est partie intégrante. C’est le temps dit “primitif”, celui des premiers hommes que nous ne connaissons quasiment pas ;</li>
<li>puis vient le temps <em>magique</em> dans lequel la figure commence à se détacher du fond mais comme un réseau de noeuds “magiques”, objets, lieux, personnes, moments, phénomènes naturels interconnectés et toujours enserrés. Les figures magiques sont les points nodaux, les “nexus”, qui font sens sur le fond du monde mais n’en sont pas séparées ;</li>
<li>cette unité magique se brise à mesure que la complexité du monde s’accroît:
<ul>
<li>les figures s’individualisent en objets techniques qui incarnent donc le processus d’objectivation,</li>
<li>le fond est universalisé en divinité, et devient le domaine du <em>religieux</em> et de la subjectivité<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> ;</li>
</ul></li>
<li>lorsque la technique est en échec, lorsque la “figure” qu’est l’objet technique trouve ses limites dans les conditions du monde, le “fond”, elle se subdivise à son tour et donne naissance à:
<ul>
<li>la <em>pensée pratique</em>, préoccupée de simplicité, d’efficacité, de la gestion de l’effort, donc figurale, locale, individuée,</li>
<li>la <em>pensée scientifique</em> préoccupée d’unité et d’universalité, de “lois naturelles”, donc un fond sur lequel s’appuyer pour développer des objets techniques et une pratique ;</li>
</ul></li>
<li><p>symétriquement la religion se divise en un fond, normatif, éthique et moral, et des figures théoriques, doctrinales, visant à l’universalité et à un savoir de type contemplatif.</p></li>
<li>l’art est un des chemins par lequel l’être humain retrouve l’unité magique: les expériences esthétiques constituent autant de <em>points remarquables</em> reliés par un réseau artistique visant à recréer du sens sur le fond du monde ;</li>
<li><p>l’autre chemin est, on s’en serait douté, celui de la philosophie dont les <em>concepts</em> réunissent les modes de pensée religieux et techniques.</p></li>
</ul>
<p>Ce modèle, discutable et discuté, a le mérite de proposer une grille de lecture riche et multiple du réel, d’articuler l’individu et le collectif, l’homme et le monde, les objets et les sujets dans un réseau de relations infiniment complexe. Il permet aussi d’argumenter sur l’importance des techniques dans la culture et sur l’amputation volontaire que constitue l’ignorance des techniques par de nombreuses personnes. Simondon appelle de ses voeux la formation d’une <em>technologie</em>, c’est-à-dire littéralement un savoir de la technique, une pensée technique, à commencer par le travail:</p>
<blockquote>
<p>L’activité technique se distingue du simple travail, et du travail aliénant, en ce que l’activité technique comporte non seulement l’utilisation de la machine mais aussi un certain coefficient d’attention au fonctionnement technique. [..] L’aliénation fondamentale [de l’homme par la machine] réside dans la rupture qui se produit enttre l’ontogénèse de l’objet technique et l’existence de cet objet technique. [..] Les objets techniques qui produisent le plus d’aliénation sont aussi ceux qui sont destinés à des utilisateurs ignorants.</p>
<p>p.339</p>
</blockquote>
<p>Il est pour moi évident que cette pensée fournit un substrat unifiant tout un ensemble de faits, mouvements, idées qui sont de plus en plus prégnants dans notre société: DIY, Makers, logiciel libre, économie collaborative et participative, open science, hackathon, Devoxx4Kids… De plus en plus nombreux sont ceux qui considèrent que reprendre possession de la technique, sur le plan pratique autant que théorique, est une nécessité, voire une urgence.</p>
<p>Dans les dernières pages de son livre Simondon critique le monde du travail, l’entreprise de son temps - et du nôtre:</p>
<blockquote>
<p>L’entreprise [..] doit être organisée à partir de sa fonction essentielle, c’est à dire de son fonctionnement technique.</p>
<p>p.343</p>
</blockquote>
<p>ainsi que l’obsession du rendement:</p>
<blockquote>
<p>Le critère de rendement ne peut pas conduire à une résolution du problème [de la zone obscure subsistant entre capital et travail] ; le rendement, par rapport à l’activité technique etst très abstrait et ne permet pas d’entrer dans cette activité pour en voir l’essence.</p>
<p>p.344</p>
</blockquote>
<p>Loin du technologisme conquérant et abstrait du <em>management scientifique</em> prôné, dans le domaine de l’organisation du travail, par Taylor et ses successeurs, prônant la séparation toujours plus fine des tâches et des rôles entre conception et réalisation, direction et exécution, pensée et action au nom de l’efficacité économique, et tout autant éloigné du scientisme positiviste aveugle, Simondon propose une philosophie de la technique à hauteur d’homme et de femme, un projet peut-être utopique mais ancré dans le réel, de réenchanté le monde en retrouvant l’unité perdue de la figure et du fond.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>C’est que Simondon nomme <em>l’hypertélie</em> : la sur-adaptation de l’objet à sa finalité.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Simondon utilise le joli mot d’hylémorphique ou hylémorphisme pour désigner ce mode de pensée issu originellement de la philosophie aristotélicienne.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>religion doit ici être entendu dans son sens le plus large, <em>ce qui relie</em>, et inclut donc les systèmes sociaux et politiques, les schèmes de pensées qui permettent d’universaliser, de relier les individus dans un tout plus grand qu’eux.<a href="#fnref3">↩</a></p></li>
</ol>
</div>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>
<entry>
    <title>Sur &quot;Technological Revolutions and Financial Capital&quot;</title>
    <link href="http://abailly.github.io/posts/techno-capital.html" />
    <id>http://abailly.github.io/posts/techno-capital.html</id>
    <published>2015-11-06T00:00:00Z</published>
    <updated>2015-11-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1>Sur &quot;Technological Revolutions and Financial Capital&quot;</h1>
<div class="info">Posted on November  6, 2015</div>

<p>Il y a quelques mois j’ai eu l’occasion d’assister à une <a href="http://conseil-developpement.loire-atlantique.fr/assaut-numerique-decrypte-devant-100-citoyens-et-professionnels-de-loire-atlantique/">conférence</a> de <a href="https://twitter.com/Nicolas_Colin">Nicolas Collin</a> donnée à l’Hôtel de Département dans le cadre du <a href="http://conseil-developpement.loire-atlantique.fr/">Comité de Développement de la Loire-Atlantique</a>. Sur les conseils de Nicolas Colin et après une discussion sur twitter, j’ai donc commandé et lu <a href="https://books.google.fr/books/about/Technological_Revolutions_and_Financial.html">Technological Revolutions and Financial Capital</a> de <a href="http://www.carlotaperez.org/">Carlota Perez</a>, professeure à la prestigieuse London School of Economics. J’en recommande la lecture et j’appelle de mes voeux une traduction qui permettrait de mieux diffuser les hypothèses et le modèle intéressant - et positif - que propose Carlota Perez.</p>
<p>Dans ce livre C.Perez propose une relecture des modèles cycliques du capitalisme (p.ex. <a href="https://fr.wikipedia.org/wiki/Cycle_de_Kondratiev">cycles de Kondratiev</a>) en articulant les (r)évolutions technologiques, financières et socio-politiques. L’hypothèse centrale du livre, soutenue par une démonstration historique, est que le capitalisme est caractérisé par des cycles longs, induits par des sauts technologiques majeurs, qui sont entrecoupés de crises plus ou moins profondes manifestant différentes phases d’intégration du nouveau paradigme. La survenue d’un nouveau paradigme technologique s’accompagne inévitablement de transformations majeures de la société dans toutes ses dimensions: économiques, sociétales, organistionelles, politiques, économiques… Ces transformations se diffusent par vague à partir d’un noyau central en provoquant des bouleversements <a href="https://en.wikipedia.org/wiki/Creative_destruction">schumpéteriens</a> dont nous subissons les effets.</p>
<p>Elle distingue dans l’histoire du capitalisme cinq paradigmes majeurs se succédant environ tous les cinquante ans:</p>
<ul>
<li>la révolution industrielle proprement dite, dont le métier à tisser automatique est l’emblème, qui démarre en Angleterre vers 1760,</li>
<li>la machine à vapeur et le train à partir de 1820, toujours depuis l’Angleterre,</li>
<li>l’acier, l’électricité et l’ingénierie, à partir de 1870 et dont le centre est l’Allemagne et les USA,</li>
<li>le pétrole, l’automobile et la production de masse, aux USA toujours, démarrant dans les années 1910-1920,</li>
<li>enfin les semi-conducteurs et technologies de l’information, dont le noyau se situe toujours aux US, à partir des années 70.</li>
</ul>
<p>La diffusion de chacun de ces paradigmes est séparée en deux grandes phases - <em>Installation</em> et <em>Déploiement</em> - séparées par un <em>point de retournement</em>, chaque phase étant elle-même subdivisée en deux sous-phases. Le modèle s’articule donc en cinq étapes caractérisées par différents modes d’interactions entre technologie, capitalisme financier et investissement productif:</p>
<ol style="list-style-type: decimal">
<li>l’<strong>Irruption</strong> dont le point de départ est un <em>big bang</em> technologique à partir duquel se déploie un faisceau de plus en plus dense d’innovations. Cette phase est celle des succès fulgurants, des innovations spectaculaires, des entrepreneurs de génies, des croissances faramineuses à mesure que le nouveau paradigme conquiert de nouveaux territoires tout en commencant à mordre sur l’ancien paradigme (toujours en place et “installé” désormais dans la routine et le conservatisme). La croissance est d’autant plus rapide qu’elle est alimentée par des flux de capitaux qui cherchent à s’employer, capitaux générés par les profits accumulés dans l’ancien paradigme mais qui ne se satisfont plus de taux de rentabilité de “pères de familles” ;</li>
<li>la <strong>Frénésie</strong> s’empare du système à mesure que le nouveau paradigme s’impose comme un nouvel <em>eldorado</em>, les investissements sont de plus en plus importants, les risques plus grands, les innovations de plus en plus risquées voires carrément frauduleuses dans le système financier. C’est l’époque des <strong>bulles spéculatives</strong>, Canal de Panama, chemins de fer transcontinentaux, Années Folles, hedge funds et CDO, des fortunes éclairs, des IPO monstrueuses… Le capital financier prend le pas sur le capital productif et l’hystérie s’auto-alimente, jusqu’à ce que…</li>
<li>le <strong>Point de retournement</strong> fasse éclater la bulle, provoquant l’effondrement du systéme financier devenu fou, et une crise généralisée plus ou moins grave et douloureuse (crise de 1890, 1929, 1973, 2008…) de l’ensemble du système économique et souvent socio-politique,</li>
<li>à cette “purge” succède la phase de <strong>Synergie</strong> qui voit se déployer le nouveau paradigme dans l’ensemble de la société. De nouvelles lois et de nouveaux modes de régulation apparaissent pour juguler la finance, un nouvel “âge d’or” s’installe tandis que l’innovation apparaît enfin sous un jour bénéfique en s’adaptant et en adaptant la structure sociale. L’ensemble des acteurs du système se convertissent au nouveau paradigme et en tirent des bénéfices, la richesse globale s’accroît et se diffuse… La <em>Belle époque</em>, le <em>New Deal</em> et les <em>Trente glorieuses</em> en sont quelques manifestations emblématiques. Durant cette phase c’est le capitalisme productif qui prend le pas sur le capitalisme financier, les ingénieurs et gestionnaires qui supplantent les financiers et les spéculateurs,</li>
<li>enfin vient le temps de la <strong>Maturité</strong> où les dernières “marches” du système sont touchées par le nouveau paradigme, où l’optimisation devient plus importante que l’innovation. Cette phase recouvre en la dissimulant plus ou moins l’irruption de la prochaine vague technologique, elle voit s’exacerber les tensions que l’expansion de la phase précédente masquait : les pauvres, moins pauvres mais plus instruits et plus exigeants réclament une part plus importante de la richesse ; le conservatisme gagne du terrain : il devient plus important de protéger ses avantages acquis que d’innover et d’en créer de nouveaux. Le capital productif trouve ses limites car les rendements décroissent, l’argent s’accumule sans trouver - encore - à s’employer…</li>
</ol>
<p>Ce qui est particulièrement intéressant dans le modèle de Carlota Perez c’est que :</p>
<ol style="list-style-type: decimal">
<li>c’est un modèle <strong>dynamique</strong> et non statique : il permet de comprendre l’évolution de notre système et non une situation particulière, aussi désirable soit-elle ;</li>
<li>il articule trois modalités du changement, technologique, économique et institutionnel, ne considérant pas qu’il y aurait une cause racine et un <em>sens de l’histoire</em> auquel chacun devrait se plier mais plutôt qu’un état de fait est la résultante d’interactions extrêmement complexes entre de nombreux acteurs ;</li>
<li>il cherche à être utile non seulement pour comprendre mais aussi pour agir : identifier les cycles et les phases devrait permettre d’ajuster au mieux les politiques publiques, pour atténuer les effets négatifs et augmenter les effets positifs de chaque étape.</li>
</ol>
<p>Je n’ai pas les compétences pour juger de la validité scientifique et historique de ce livre. “Tous les modèles sont faux, certains sont utiles” disait <a href="https://en.wikiquote.org/wiki/George_E._P._Box">G.Box</a> et le peu que je connais des différents domaines qu’il couvre m’incite à penser que ce modèle particulier fait partie de la seconde catégorie. Les question qu’il m’incite à me poser sont les suivantes:</p>
<ol style="list-style-type: decimal">
<li>si nous suivons ce modèle, dans quelle phase nous situons nous actuellement et qu’est ce que cela implique pour la société et l’action publique ? Certains semblent penser que nous nous trouvons dans la phase de Synergie du paradigme <em>technologie de l’information</em> dont le point de retournement aurait été la bulle Internet des années 2000 suivi par la crise de 2008, ce qui signifierait que l’on assiste au déploiement du nouveau paradigme destiné à supplanter l’ancien à bref échéance ;</li>
<li>si le <a href="https://en.wikipedia.org/wiki/Accelerating_change">changement accélère</a>, le modèle ne devient-il pas caduque car il supppose pour chacune des phases un temps plus ou moins incompressible nécessaire ?</li>
</ol>


<div id="disqus_thread"></div>
<script>
  (function() {  // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
  
    s.src = '//arnaudsblog.disqus.com/embed.js';
  
    s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


]]></summary>
</entry>

</feed>
